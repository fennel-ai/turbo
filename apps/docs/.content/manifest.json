{"getting-started/welcome-to-fennel":{"title":"Welcome to Fennel","slug":"welcome-to-fennel","path":"/Users/lukesmetham/Code/fennel/turbo/apps/docs/.content/md/README.md","content":"---\ndescription: The modern realtime feature engineering platform\n---\n\n# Welcome to Fennel\n\nFennel is a modern realtime feature engineering platform and has been architected from the ground up in service of three design goals.\n\n### Fennel's Three Design Goals\n\n1. **Easy to install, learn & use** - **** using familiar Python instead of special DSLs, simple but powerful abstractions, zero dependency installation, fully managed with zero ops, same code working for both realtime and non-realtime cases and more to make using Fennel as easy as possible\n2. **Reduce cloud costs** - being significantly lower on cloud costs compared to other alternatives by squeezing as much out of cloud hardware as possible \\[See [this](overview/cost-optimizations.md) for how Fennel does this]\n3. **Encourage best practices** - native support for testing, CI/CD, versioned & immutable features, lineage tracking, enforcement of code ownership, data expectations, read/write compute separation and more to help you bring best engineering practices to feature engineering too. In other words, Fennel tries to replicate for feature engineering what dbt has done for broader data engineering.\n\nAs a result of the architectural philosophy, Fennel ends up unlocking the following benefits:\n\n### Benefits of Fennel\n\n* **Higher development velocity**: more iterations can be done in the same time leading to higher business value&#x20;\n* **Lower total costs of ownership**: Fennel saves costs across the board - cloud spend, bandwidth of engineers that would have gone in ops, and bandwidth of data scientists by making them more productive\n* **Higher business value via realtime features:** unlocking **** realtime and other sophisticated features leads to better models with higher business gains\n* **Healthier codebase & more reliable features**: engineering best practices like testing, immutability, code ownership etc improve code maintainability leading to more reliable data & feature\n\n### Getting Started With Fennel\n\n[Start](getting-started/quickstart.md) here if you want to directly dive deep into an end-to-end example.&#x20;\n\nOr if you are not in a hurry, read about the main [concepts](overview/concepts/) first followed by some more details about the [datasets](quick-start/examples/) and [featuresets](broken-reference). And if you run into any issues or have any questions/feedback, you're welcome to jump into our slack channel to directly chat with the engineers building it.&#x20;\n\nWe, the team behind Fennel, have thoroughly enjoyed building Fennel and hope learning and using Fennel brings as much delight to you as well!\n"},"getting-started/why-fennel":{"title":"Why Fennel?","slug":"why-fennel","path":"/Users/lukesmetham/Code/fennel/turbo/apps/docs/.content/md/getting-started/why-fennel.md","content":"# Why Fennel?\n\n## The Problem\n\nFeature engineering is hard -- data is spread over multitude of data sources like Postgres/MySQL, Kafka streams, Warehouse, S3 etc. Each of these have their own performance & data freshness characteristics and their own tooling to work with them.&#x20;\n\nBut features need to be written over all this data, plumbed in complex ways, served online with low latency, and maintained/developed by a team of people. Features also evolve, get deprecated, and run into data quality issues. All of it gets significantly harder when even a small number of realtime features are needed.\n\nHere are some of the top problems that teams run into when trying to ship ML features in production:\n\n| Authoring                                                                       | Governance                                            | Operations                             | Realtimeliness                                                         |\n| ------------------------------------------------------------------------------- | ----------------------------------------------------- | -------------------------------------- | ---------------------------------------------------------------------- |\n| Preferable to write Python over DSL or Spark/Flink pipelines                    | Hard to keep data & feature quality in check          | Ops overhead of lots of moving pieces  | Updating features in seconds with fresh data                           |\n| Hard to test feature pipelines and do CI/CD                                     | Lineage tracking for privacy                          | Very high cloud costs                  | Low latency (millisecond) level serving                                |\n| Hard to write features over data from multiple streaming and batch data sources | Tracking and fixing older or deprecated data/features | Security best practices for compliance | Keeping online served features & offline features for training in sync |\n\n## Fennel's Approach\n\nThis is where Fennel comes in. Fennel is a modern feature engineering platform that simplifies the full life cycle of feature engineering - from authoring/updating, computation, serving, and monitoring. Fennel obsesses over the following, so you don't have to:\n\n### Experience\n\n* **Authoring** - features & pipeline authoring in Python using Pandas and other familiar libraries, instead of custom DSLs or YAML configs which are \"sufficient\" until they are not.\n* **Powerful join capabilities** - do powerful joins (even on streaming data!) so you don't need to denormalize/enrich your data every time you add new features\n* **Data Connectors –** pre-built data connectors to ingest data from batch and streaming sources like Postgres, S3, Snowflake, BigQuery, Kafka, and more.\n\n### Power\n\n* **Realtime –** support real-time features with minimum lag between the time an event is ingested and when feature values are updated as a result.\n* **Blazing Fast** – support high-throughput low-latency reads (p99 of single digit millisecond)\n* **Horizontally scalable** - horizontally scale to billions of events and feature reads per day\n\n### Quality\n\n* **Best engineering practices** - **** native support for **** unit & integration testing even for complex pipelines and features and CI/CD\n* **Focus on correctness** - all code assets are versioned and/or immutable to prevent accidental changes, everything is strongly typed despite being Python native\n* **Data quality checks \\[coming soon]** - **** inbuilt support for specifying/tracking data expectations and drift monitoring\n\n### Operations\n\n* **Zero dependency install** - installs inside your VPC in minutes with zero dependencies, no need to bring your own Spark, Kafka, Warehouse, Redis etc. &#x20;\n* **Fully managed ops** - 99.99% uptime SLA without thinking about machines, uptimes, autoscaling etc.&#x20;\n* **Minimal Cloud Costs** - keep cloud costs as low as possible and pass all savings (without margin) to the customers\n"},"getting-started/installation":{"title":"Installation","slug":"installation","path":"/Users/lukesmetham/Code/fennel/turbo/apps/docs/.content/md/quick-start/installation.md","content":"# Installation\n\n## 1. Install the client library\n\n```bash\npip install fennel-ai\n```\n\nTo find the version of the installed library\n\n```\npip show fennel-ai\n```\n\n_To try out the abstraction without a real cluster checkout using a mock client under_ [_Unit Tests_](../testing/unit-tests.md)__\n\n\n\n## 2. Provision a cluster\n\nCurrently, the self-serve option to create a cluster doesn't exist. Instead, ask your contact at Fennel to create a dedicated cluster for you and give you its URL. Also, see [deployment model](../overview/deployment-model.md) to learn more about how Fennel cluster can be provisioned inside your VPC.\n\n"},"getting-started/quickstart":{"title":"Quickstart","slug":"quickstart","path":"/Users/lukesmetham/Code/fennel/turbo/apps/docs/.content/md/getting-started/quickstart.md","content":"# Quickstart\n\n```python\nfrom fennel.datasets import dataset, field, pipeline, Dataset\nfrom fennel.lib.aggregate import Count\nfrom fennel.lib.metadata import meta\nfrom fennel.lib.schema import Embedding, Series\nfrom fennel.lib.window import Window\n\n# Step 1: define connectors to your data sources. Here \n# it is done in the UI (so that credentials don't have to be \n# put in code) and referred by the names given to them\npostgres = sources.Postgres.get(name='my_rdbms')\nwarehouse = sources.Snowflake.get(name='my_warehouse')\n\n# Step 2: define a few datasets - each dataset has some\n# typed fields, an owner, and can optionally be given tags\n# datasets can be sourced from sources defined in step 1\n@dataset\n@source(postgres.table(\"merchant_info\", cursor=\"last_modified\"), every='1m')\n@meta(owner=\"aditya@fennel.ai\")\nclass MerchantInfo:\n    merchant_id: int = field(key=True)\n    merchant_category: str\n    merchant_city: int\n    merchant_num_employees: int\n    created_on: datetime\n    last_modified: datetime = field(timestamp=True)\n\n@dataset\n@source(postgres.table('user_info', cursor='last_modified'), every='1m')\n@meta(owner='nikhil@fennel.ai', tags=['PII'])\nclass UserInfo:\n    user_id: int = field(key=True)\n    name: str\n    gender: oneof(str, ['male', 'female'])\n    dob: str\n    age: int\n    created_on: datetime\n    country: Optional[str]\n    last_modified: datetime = field(timestamp=True)\n    \n    @expectations\n    def get_expectations(cls):\n      return [\n        expect_column_values_to_be_between(\n          column=str(cls.age),\n          min_value=13,\n          max_value=100,\n          mostly=0.95\n      )]\n\n# Here Fennel brings data from warehouse. This way, Fennel\n# let you bring data from disparate systems in the same plane\n# which makes it easier to work across many data sysetms\n@dataset\n@source(warehouse.table(\"user_activity\", cursor_field='timestamp'), every='15m')\n@meta(owner='luke@fennel.ai')\nclass Activity:\n    user_id: int\n    action_type: float\n    amount: Optional[float]\n    metadata:  str\n    timestamp: datetime\n\n@dataset\n@meta(owner='laura@fennel.ai')\nclass FraudReportAggregateByCountry:\n    merchant_id: int = field(key=True)\n    country: str = field(key=True)\n    timestamp: datetime\n    num_merchant_country_fraud_transactions: int\n    num_merchant_country_fraud_transactions_7d: int\n\n    # Fennel lets you write pipelines that operate on one or more\n    # datasets - you get SQL like power but with arbitrary Python.\n    # and the same pipeline code works for batch & streaming data alike!\n    @pipeline(Activity, UserInfo)\n    def create_fraud_dataset(activity: Dataset, user_info: Dataset):\n        def extract_info(df: pd.DataFrame) -> pd.DataFrame:\n            df['metadata_dict'] = df['metadata'].apply(json.loads).apply(pd.Series)\n            df['transaction_amount'] = df['metadata_dict'].apply(lambda x: x['transaction_amt'])\n            df['merchant_id'] = df['metadata_dict'].apply(lambda x: x['merchant_id'])\n            return df[['merchant_id', 'transaction_amount', 'user_id']]\n\n        filtered = activity.filter(lambda r: r[\"action_type\"] == \"report_txn\")\n        extracted = filtered.transform(extract_info)\n        joined = extracted.join(user_info, on=[\"user_id\"])\n        return jonined.groupby(\"merchant_id\", \"country\").aggregate([\n            Count(window=Window(\"forever\"), into_field=\"num_merchant_country_fraud_transactions\"),\n            Count(window=Window(\"1w\"), into_field=\"num_merchant_country_fraud_transactions_7d\"),\n        ])\n\n# Step 3: define some featuresets - each featureset\n# is a collection of logically related features. Features\n# can be added/removed from featuresets following a tagging\n# mechanism similar to protobufs and each feature itself is\n# immutable once created\n@featureset\n@meta(owner='anti-fraud-team@fennel.ai')\nclass Merchant:\n    merchant_id: int = feature(id=1)\n    merchant_category: int = feature(id=2)\n    merchant_city: int = feature(id=3)\n    merchant_age: int = feature(id=4)\n    merchant_num_employees: int = feature(id=5)\n\n    # Fennel lets you specify code that knows how to\n    # extract one or more features of a featureset\n    @extractor\n    @depends_on(MerchantInfo)\n    def get_merchant_info(ts: Series[datetime], merchant_id: Series[merchant_id]):\n        df = MerchantInfo.lookup(ts, merchant_id=merchant_id)\n        df[\"current_timestamp\"] = ts\n        df[\"merchant_age\"] = df.apply(\n            lambda x: x[\"current_timestamp\"] - x[\"created_on\"], axis=1)\n        return df\n\n@featureset\n@meta(owner=\"abhay@fennel.ai\")\nclass MerchantBehaviorFeatures:\n    num_merchant_country_fraud_transactions: int = feature(id=1)\n    num_merchant_country_fraud_transactions_7d: int = feature(id=2)\n    fradulent_transaction_ratio: float = feature(id=3)\n\n    @extractor\n    @depends_on(FraudReportAggregateByCountry)\n    def get_merchant_fraud_features(\n        ts: Series[datetime],\n        country: Series[User.country],\n        merchant_info: DataFrame[Merchant]) -> DataFrame[\n            num_merchant_country_fraud_transactions,\n            num_merchant_country_fraud_transactions_7d,\n            fradulent_transaction_ratio]:\n        df = FraudReportAggregateByCountry.lookup(\n            ts, merchant_id=merchant_info['merchant_id'], country=country)\n        df['fradulent_transaction_ratio'] = df.apply(\n            lambda x: x.num_merchant_country_fraud_transactions/x.num_merchant_country_transactions,\n            axis=1)\n        return df\n\n```\n"},"overview/concepts":{"title":"Concepts","slug":"concepts","path":"/Users/lukesmetham/Code/fennel/turbo/apps/docs/.content/md/overview/concepts/README.md","content":"# Concepts\n\nFennel has two main concepts -- datasets and featuresets. Let's look at both one by one\n\n### 1. Dataset\n\nDataset refers to a \"table\" of data with typed columns. Duh! Here is how a dataset is defined.&#x20;\n\n```python\n@dataset\nclass User:\n    uid: int = field(key=True)\n    dob: datetime\n    country: str\n    update_time: datetime = field(timestamp=True)\n```\n\nThis dataset has four columns -- `uid` (of type int), `dob` (of type datetime),`country` (of type string), and `signup_time` (of type datetime). For now, ignore the `field(...)` descriptors - they'd be explained soon.&#x20;\n\nHow to get data into a dataset? It's possible to hydrate datasets from external data sources:\n\n```python\nfrom fennel import source\nfrom fennel.sources import Postgres, Kafka\n\npostgres = Postgres(host=...<credentials>..)\nkafka = Kafka(...<credentials>..)\n\n@source(postgres.table('user'), every='1m')\n@dataset\nclass User:\n    uid: int\n    dob: datetime    \n    country: str\n    signup_time: datetime\n\n@source(kafka.topic('transactions'))\n@dataset\nclass Transaction:\n    uid: int\n    amount: float\n    payment_country: str\n    merchant_id: int\n    timestamp: datetime\n```\n\nThe first dataset will poll postgres table for new updates every minute and hydrate itself with new data. The second dataset hydrates itself from a kafka topic. Fennel supports connectors with all main sources - check [here](broken-reference) for details.&#x20;\n\nHydrating datasets this way from external sources already looks somewhat cool because it allows you to bring data from multiple places in the same abstraction layer. But what to do with these datasets?\n\nFennel lets you derive new datasets from existing datasets by writing simple declarative Python code - it's unimaginatively called a pipeline. Let's look at one such pipeline:\n\n```python\n@dataset\nclass UserTransactionsAbroad:\n    uid: int = field(key=True)\n    count: int\n    amount_1d: float\n    amount_1w: float\n    \n    @pipeline(User, Transaction)\n    @classmethod\n    def first_pipeline(cls, user: Dataset, transaction: Dataset):\n        joined = transaction.join(user, on=['uid'])\n        abroad = joined.filter(lambda df: df['country'] != df['payment_country'])\n        return abroad.groupby(['uid']).aggregate(\n            Count(window='forever', into_field='count'),\n            Sum(of='amount', window='1d', into_field='amount_1d'),\n            Sum(of='amount', window='1d', into_field='amount_1w'),\n        )\n```\n\nThis is a dataset that will keep rolling stats about transactions made by a user abroad and we want to derive it from `User` dataset and `Transaction` dataset. Line 8-17 define this pipeline. You'd note that this pipeline is written using native Python and Pandas so you can unleash the full power of Python. But more importantly, this pipeline is operating on two datasets, one of which is streaming (i.e. `Transaction` ) and comes from Kafka and the other is static-ish dataset (i.e. `User`) coming from Postgres. And you can do joins and aggregations across them both. Wow! Now this is beginning to look powerful. What else can you do with the datasets?\n\nIt's also possible to do low latency lookups on these datasets using dataset keys. Earlier you were asked to ignore the field descriptors -- it's time to revisit those. If you look carefully, line 3 above defines `uid` to be a key (dataset can have multi-column keys too). If we know the uid of a user, we can ask this dataset for the value of the rest of the columns. Something (but not exactly) like this:\n\n```python\nuids = pd.Series([1, 2])\ndata, found = UserTransactionsAbroad.lookup(ts=[now, now-10], uid=uids)\n```\n\nHere \"found\" is a boolean series denoting whether there was any row in the dataset with that key. If data isn't found, a row of Nones is returned. What's even cooler is that this method can be used to lookup the value as of any arbitrary time -- Fennel datasets track time evolution of data as the data evolves and can go back in time to do a lookup. This movement of data is tagged with whatever field is tagged with `field(timestamp=True)`. In fact, this ability to track time evolution enables Fennel to use the same code to generate both online and offline features.&#x20;\n\nOkay so we can define datasets, source them from external datasets, derive them via pipelines, and do complex temporal lookups on them. What has all this to do with features? How to write a feature in Fennel? Well, this is where we have to talk about the second main concept -- featureset\n\n### 2. Featureset\n\nA featureset, as the name implies, is just a collection of features, each with some code that knows how to extract it - called an _extractor_. That may sound like a mouthful but isn't that complicated. Let's define a feature that computes user's age using the datasets we defined above.\n\nHere is how a really simple featureset looks:\n\n```python\n@featureset\nclass UserFeature:\n    uid: int = feature(id=1)\n    country: str = feature(id=2)\n    age: float = feature(id=3)\n    \n    @extractor\n    def get_age(cls, ts: Series[datetime], uids: Series[uid]) -> Series[age]:\n        dobs = User.lookup(ts=ts, uid=uids, fields=['dob'])\n        ages = [dob - datetime.now() for dob in dobs]\n        return pd.Series(ages)\n        \n    @extractor\n    def get_country(cls, ts: Series[datetime], uids: Series[uid]) -> Series[country]:\n        countries, _ = User.lookup(ts=ts, uid=uids, fields=['country'])\n        return countries\n```\n\nThis is a featureset with 3 features -- `uid`, `country`, and `age`. Lines 7-11 describe an extractor that given the value of the feature `uid`, knows how to define the feature `age` (this input/output information is encoded in the typing signature, not function names). Inside the extractor function, you are welcome to do arbitrary Python computation. Similarly, lines 13-16 define another extractor function, this time which knows how to compute `country` given the input `uid`.&#x20;\n\nMore crucially, these extractors are able to do lookup on `User` dataset that we defined earlier to read the data computed by datasets. That's it - that's the basic anatomy of a featureset - one or more typed features with some extractors that know how to extract those features. These features extractors can recursively depend on other features (whether in the same featureset or across) and know how to compute the output features.&#x20;\n\nAt this point, you may have questions about the relationship between featuresets and datasets and more specifically pipeline and extractor.&#x20;\n\nThe main difference is that datasets are updated on the write path -- as the new data arrives, it goes to datasets from which it goes to other datasets via pipelines. All of this is happening asynchronously and results in data being stored in datasets. Features, however, are a purely 'read side' concept - feature is extracted while the request is waiting (this could be online feature serving request or offline training data generation request). Features can recursively depend on other features. And the bridge between them is `lookup` functionality of the dataset.&#x20;\n\nHere is a diagram of how the concepts fit together:\n\n<figure><img src=\"../../.gitbook/assets/image (1).png\" alt=\"\" /><figcaption></figcaption></figure>\n\nThis provides a relatively simplified bird's eye view of the main concepts. But there is more to both datasets and featuresets and how they come together. You can read in more detail about [datasets here](broken-reference) and about [featuresets here](broken-reference).\n\n### Syncing Datasets and Features with Fennel\n\nWhen you work with Fennel, your datasets and featuresets will live in a Python file in your codebase and Fennel servers will not know about them until you inform the servers by issuing a `sync` call. Here is how it will look:\n\n```python\nfrom fennel.client import Client\n\nclient = Client(<FENNEL SERVER URL>)\nclient.sync(\n    datasets=[User, Transaction, UserTransactionsAbroad],\n    featuresets=[UserFeature],\n)\n```\n\nLine 4 here makes a POST request to Fennel and syncs the dataset on the server. Fennel may reject this sync request if there is any error with any dataset or featureset e.g. if a dataset already exists with this name or somehow this dataset is malformed.&#x20;\n\nOvertime, you'd have many more datasets and featuresets - you'd send all of them in a sync call. And with that, the validation can become lot more complex e.g schema compatibility validation across the whole graph of datasets/featuresets\n\nAssuming the call succeeds, any datasets/featuresets that don't yet exist will be created, any datasets/featuresets that exist but are not provided in the sync call are deleted and rest are left unchanged. See the [section on CI/CD](../../testing-and-ci-cd/ci-cd-workflows.md) to learn how the end to end deployment could work in a production environment\n\n"},"overview/deployment-model":{"title":"Deployment Model","slug":"deployment-model","path":"/Users/lukesmetham/Code/fennel/turbo/apps/docs/.content/md/overview/deployment-model.md","content":"# Deployment Model\n\nThe primary deployment model is to run Fennel in a fully managed mode inside your VPC as a single tenant system \\[1] -- as a result, the data never leaves your cloud and is fully covered by your existing infosec policies.&#x20;\n\nOver time, a hosted multi-tenancy system will also be launched.&#x20;\n\n\n\n**How does deployment work inside your VPC?**\n\nYou create a sub-organization in your cloud and give Fennel admin privileges to it. Fennel's automation can bring up machines/services and scale things up and down as needed to handle the traffic. Fennel cluster inside your VPC talks to Fennel control plane to exchange telemetry information and control instructions (e.g. control plane may choose to downscale the dataplane when traffic is lower, say at night). Either way, your data doesn't leave your cloud premise.\n\nSince Fennel is limited to a sub-organization, it has no visibility / access into the rest of your cloud.&#x20;\n\n\n\n\\[1] If it makes your life easier, we are also happy to spin up a separate AWS organization ourself and run your system inside it. That way, you'd effectively get a single tenant system outside of your VPC.\n"},"overview/architecture":{"title":"Architecture","slug":"architecture","path":"/Users/lukesmetham/Code/fennel/turbo/apps/docs/.content/md/overview/architecture.md","content":"# Architecture\n\nHere are some of the key ideas & principles behind Fennel's architecture that allow it to meet its design goals of  1) being easy to install, learn, use 2) being cost effective 3) and encouraging eng best practices.&#x20;\n\n### Read Write Separation\n\nThis is arguably the most critical architecture decision that differentiates Fennel from many other similar systems. You can read about this in more detail [here](read-write-separation.md).\n\n### Kappa Architecture\n\nFennel uses a Kappa like architecture to operate on streaming and offline data. This enables Fennel to maintain a single code path to power the data operations and have the same pipeline declarations work seamlessly in both realtime and batch sources. This side-steps a lot of issues intrinsic to the Lambda architecture, which is the most mainstream alternative to Kappa.\n\n### Hybrid Materialized Views\n\nTo reduce read latencies, data on the write path is pre-materialized and stored in datasets. The general downside of materializing views is that it may lead to wasted computation and storage for data that is never read. Fennel's read write separation minimizes this downside by giving control to the end user for what computation to pre-materialize and what computation to on the read path.\n\n### Minimal Sync Communication for Horizontal Scaling\n\nEvery single subsystem within Fennel is designed with horizontal scalability in mind. While this is mostly a positive, if not done well, lots of independent nodes can lead to inter-node overheads leading to capacity of the system not growing linearly with hardware capacity. It also creates failure modes like cascades of failures.&#x20;\n\nFennel minimizes these by reducing cross-node sync communication - it does so by keeping some local state with each node, keeping global metadata in centrally accessible Postgres, and making the rest of the communication async mediated via Kafka (vs sync RPCs)\n\n### Async Rust (using Tokio)\n\nAll Fennel services are written in Rust with tight control over CPU and memory footprints. Further, since feature engineering is a very IO heavy workload on both read and write sides, Fennel heavily depends on async Rust to release CPU for other tasks while one task is waiting on some resource. For instance, each `job` in Fennel's streaming system gets managed as an async task enabling many jobs to share the same CPU core with minimal context switch overhead. This enables Fennel to efficiently utilize all the CPU available to it.\n\n### Embedded Python\n\nFennel's philosophy is to let users write in real Python with their familiar libraries vs having to learn new DSLs. But this requires lot of back and forth between Python land (which is bound by GIL) and the rest of the Rust system. Fennel handles this by embedding a Python interpreter inside Rust binaries (via PyO3). This enables Fennel to cross the Python/Rust boundary very cheaply, while still being async.&#x20;\n"},"overview/read-write-separation":{"title":"Read/Write Separation","slug":"read-write-separation","path":"/Users/lukesmetham/Code/fennel/turbo/apps/docs/.content/md/overview/read-write-separation.md","content":"# Read/Write Separation\n\nUnlike most (if not all?) other feature platforms out there, Fennel distinguishes between read and write path computation. This is so because these are very different from perf characteristics -&#x20;\n\n* Write path is throughput bound but does not have latency pressure whereas Read path is very sensitive to latency but often operates on tiny batches.&#x20;\n* Write path creates data and so can end up taking lots of storage space, some of which may be wasted if the data isn't actually read in a request. That storage space may be traded off with CPU by moving that computation to the read path which may repeat the same computation for each request.\n\nWhat computation to do on write path vs read path is a very application specific decision and so Fennel gives you control on that decision - pre-compute some quantity on the write path by keeping it in a pipeline. Or compute it on the fly by writing it as an extractor.&#x20;\n\nNote that in a majority of cases, you'd just want to move the computation to write side as a pipeline. But there are several reasons/cases where keeping the computation on read path may make sense. Here are some examples:\n\n* **Sparsity** -- say you have a feature that does dot product between user and content embeddings. There are 1M users and 1M content. It's very wasteful to compute dot product between every pair of user/content because it will take a lot of storage and most of it will not be read ever. So it's better to lookup embeddings from datasets and do their dot product in the extractor.\n* **CPU cost** -- say you have a model based feature and further it's a heavy neural network model. You could put it on the write path to save latency. But that also means that you're running this neural network on every row of dataset, most of which may never be seen in prod. So depending on the CPU vs latency tradeoff, it may make sense to keep it on the read path.&#x20;\n* **Request properties** -- for features that depend on request specific properties e.g. \"is the transaction amount larger than user's average over the last 1 day\", you may have no choice but to look up some partially pre-computed info and do some read side computation it.&#x20;\n* **Temporal features** - say you have a feature that represents user's age. The values of this feature update automatically with passing of time and so it's physically impossible to pre-compute this on the write path. A more natural way is to look up user's date of birth from a dataset and subtract that from the current time.\n"},"overview/cost-optimizations":{"title":"Cost Optimizations","slug":"cost-optimizations","path":"/Users/lukesmetham/Code/fennel/turbo/apps/docs/.content/md/overview/cost-optimizations.md","content":"# Cost Optimizations\n\nKeeping cloud costs low is one of the top 3 design goals for Fennel (the other two being ease of use and encouraging best practices).&#x20;\n\nFennel delivers on this goal by investing in advanced cost optimizations -- optimizations which which are harder to invest in for each company individually but become feasible due to economics of scale enjoyed by Fennel. Together, these optimizations significantly reduce cloud spend for the same workloads.&#x20;\n\nHere is a non-exhaustive list of such optimizations:\n\n* Keeping services stateless whenever possible and then using spot instances\n* Serve features using RocksDB based disk/RAM hybrid instead of more costly options like DynamoDB or Redis (which keeps all data in RAM)\n* Using minimal managed services provided by cloud vendors and instead running the open source versions on our own on top of just the EC2 (this avoid 2-3x markup charged by cloud vendors)\n* Using efficient Rust to power all services to reduce CPU demands\n* In particular, not relying on more general purpose streaming systems like spark or Flink but using an in-house Rust based system purpose built for feature engineering workloads with much lower overhead\n* Using AWS graviton processor based instances which offer better price/performance ratio\n* Auto scaling up/down various clusters depending on the workload (e.g. reduce costs at night)\n* Tightly encoding data in binary formats (e.g. using variable length ints etc.) in all storage engines to reduce storage and network bandwidth costs\n* Adding compression (say at disk block level) in data storage systems\n* Data tiering - preferring to keeping data in S3 vs instance store vs RAM whenever possible\n* Avoiding network costs by preferably talking to data sources in the same AZ\n* Avoiding memory copies and more generally keeping memory footprint predictably low during both serving and write side pipelines\n* Not relying on Spark or other JVM based systems that can do a lot of data shuffling and hence need lots of RAM\n* ...and lot more\n\n\n\n\n\n\\\n"},"datasets/overview":{"title":"Overview","slug":"overview","path":"/Users/lukesmetham/Code/fennel/turbo/apps/docs/.content/md/quick-start/examples/README.md","content":"# Overview\n\nDatasets refer to a table like data with typed columns. Datasets can be sourced from external datasets (e.g. Kafka, Snowflake, Postgres etc.) or derived from other datasets.&#x20;\n\nDatasets are written as [Pydantic](https://docs.pydantic.dev/) inspired Python classes decorated with the `@dataset` decorator.&#x20;\n\n### Example\n\n```python\n@dataset\nclass User:\n    uid: int = field(key=True)\n    dob: datetime\n    country: str\n    update_time: datetime = field(timestamp=True)\n```\n\n### Dataset Schema\n\nA dataset has few typed columns (interchangeably referred to as fields) and unique names. Each field must have has a pre-specified datatype. See the [typing](../../api-reference/data-types.md) section to learn the types supported by Fennel.&#x20;\n\n### Field Descriptors\n\nYou might have noticed the `field(...)` descriptor next to `uid` and `update_time` fields. These optional descriptors are used to provide non-typing related information about the field. In particular, there are three kinds of fields:\n\n1. `key` fields - these are fields with `field(key=True)` set on them. The semantics of this are somewhat similar to those of primary key in relational datasets and implies that datasets can be looked-up by providing the value of key fields. It is okay to have a dataset with zero key fields - in those cases, it's not possible to do random lookups on the dataset at all. Typically realtime activity streams (e.g. click streams) will not have any key fields. It's also okay to have multiple key fields on a dataset - in that case, all of those need to be provided while doing a lookup. And since keys are tied to lookup, they can not be of an Optional type.\n2. `timestamp` field - these are fields with `field(timestamp=True)`. Every dataset should have exactly one timestamp field and this field should always be of type `datetime`. Fennel datasets automatically track data mutations over time which is needed to be able to compute point-in-time correct features for training data generation. It's the value of the `timestamp` field that is used to associate a particular state of dataset row with a timestamp. While every dataset has exactly one timestamp field, it's possible to omit it in code - if a dataset has exactly one field with `datetime` type, it is assumed to be the timestamp field.&#x20;\n\n<Hint type=\"info\">Timestamp fields of datasets, in addition to time travel, also allows Fennel to handle out of order events and do time-windowed data aggregations</Hint>\n\nHere are some examples of valid and invalid datasets:\n\n```python\n# valid - has no key fields, which is fine. \n# no explicitly marked timestamp fields so update_time, which is of type\n# datetime is automatically assumed to be the timestamp field\n@dataset\nclass User:\n    uid: int\n    country: str\n    update_time: datetime\n\n# invalid - key fields can not have an optional type\n@dataset\nclass User:\n    uid: Option[int] = field(key=True)\n    country: str\n    update_time: datetime        \n    \n# invalid - no field of `datetime` type\n@dataset\nclass User:\n    uid: int\n    country: str\n    update_time: int\n\n# invalid - no explicitly marked `timestamp` field\n# and multiple fields of type `datetime` so timestamp \n# field is amgiguous\n@dataset\nclass User:\n    uid: int\n    country: str\n    created_time: datetime\n    updated_time: datetime\n```\n\n### Meta Flags\n\nDatasets can be annotated with useful meta information - either at the dataset level or at the single field level. And the same metaflags that work elsewhere in Fennel also work on datasets. Read more about [metaflags here](../../governance/metaflags.md). Here is an example:\n\n```python\n@meta(owner='abc-team@fennel.ai', tags=['PII', 'experimental'])\n@dataset\nclass User:\n    uid: int = field(key=True)\n    height: float = field().meta(description='height in inches')\n    weight: float = field().meta(description='weight in lbs')\n    updated: datetime\n```\n\nQuick note - to encourage code ownership, owner metaflag MUST be present on every dataset and featureset - else the sync call will fail. Fennel uses this to notify the owner of the Dataset in case of any issues (e.g. data quality problems, some upstream dataset getting marked deprecated etc.)\n\nHowever, these are omitted in spirit of brevity and clarity in several examples throughout the documentation.&#x20;\n\n### Learn more about datasets\n\nDatasets, despite being a very simple and compact abstraction, pack a punch in terms of power. Here are a few topics to read next to learn more about datasets.&#x20;\n\n* [Bringing data into a Dataset from external data sources](../../datasets/sources.md)\n* [Writing pipelines to derive datasets from existing datasets](../../api-reference/datasets/pipelines/)\n* [Reading a Dataset by doing lookups](../../api-reference/datasets/pipelines/)\n"},"datasets/sources":{"title":"Sources","slug":"sources","path":"/Users/lukesmetham/Code/fennel/turbo/apps/docs/.content/md/datasets/sources.md","content":"# Sources\n\nOne of the central challenges of feature engineering is that the data is spread over data systems of different modalities - everything from Postgres, Kafka, S3, to even analytical systems like Redshift, Bigquery, Snowflake etc. Each of these have different perf characteristics, different states of data freshness, and their own custom tooling. All of this makes it hard to write features across all of these. &#x20;\n\nFennel datasets allow you to bring relevant data from all of these and more sources into the same \"plane of abstraction\" to be able to write various features. It does so by providing pre-built sources that know how to read data from each of these systems. Let's look at an example to see how it works:\n\n### **Example**\n\n```python\nfrom fennel.sources import source, Postgres\n\npostgres = Postgres(host=...<credentials>..)\n\n@source(postgres.table('user'), cursor='update_time', every='1m')\n@meta(owner='xyz@example.com')\n@dataset\nclass UserLocation:\n    uid: int\n    city: str\n    country: str\n    update_time: datetime    \n```\n\nIn this example, line 3 creates an object that describes 'connection' to your Postgres database. Line 6-12 describe a dataset that needs to be sourced from the Postgres. And line 5 declares that this dataset should be sourced from a table named `user` within the Postgres database. And that's it - once this is written, `UserLocation` dataset will start mirroring your postgres table `user`and will update as the underlying Postgres table updates.&#x20;\n\nEach source has its own configuration options. In case of Postgres, there are two more options besides the table name as shown in this example- `cursor`  and `every`. Let's look at both one by one:\n\n**Cursor**\n\nOnce this dataset is declared, Fennel will periodically ask Postgres if there have been any new rows since the last time it checked. The way it does this is by remembering the last value of cursor column (in this case `update_time`) and issuing a query of the form `SELECT * FROM user WHERE update_time > last_update_time.` As you might have noticed, for this to work the cursor field needs to be monotonically increasing with row updates. As long as that is the case, Fennel will source every new row into the dataset. Several data sources have a cursor parameter except for some (e.g. Kafka/S3) which have a cursor like functionality built into the data source itself (e.g. offsets in Kafka act as natural cursor)\n\n**Every**\n\n`every` denotes the period after which Fennel asks Postgres for new rows. Almost all sources expose this parameter using which you can control the frequency at which data is synced.\n\n### Schema Matching\n\nIt is expected that the names of fields in the dataset match the schema of the external data source. In this example, for instance, it is expected that the `user` table in Postgres will have at leat four columns -`uid`, `city`, `country`, and `update_time` each with appropriate types. The postgres table could totally have many more columns - they are simply ignored.&#x20;\n\nHere is how various types are matched from the sourced data:\n\n* `int`, `float`, `str`, `bool` match with any integer types, float types, string types and boolean types respectively. For instance `int` type checks with INT8 or UINT32 from Postgres.&#x20;\n* `List[T]` matches a list of data of type T and `Dict[T]` matches any data that is dictionary from strings to columns of type T.&#x20;\n* `Option[T]` on dataset matches with column if either the cell is `null` or if its non-null value matches type T.\n* `datetime` matching is a bit more flexible to support common date formats and is defined below.\n\nMismatch of sourced data schema and the schema of Fennel datasets is a runtime error though we are working towards moving some of that error checking to \"compile\" time when the datasets are first declared.&#x20;\n\n### Datetime Parsing\n\nIf a dataset field is declared to be of type `datetime` (as is `update_time` in the example above), it can be safely parsed from any of the following data:\n\n* Integers that describe timestamp as seconds/milliseconds/microseconds/nanoseconds from Unix epoch. Fennel is smart enough to automatically deduce if an integer is describing timestamp as seconds, miliseconds, microseconds or nanoseconds.\n* Strings that describe timestamp in [RFC 3339](https://www.ietf.org/rfc/rfc3339.txt) format e.g. `'2002-10-02T10:00:00-05:00'` or `'2002-10-02T15:00:00Z'` or `'2002-10-02T15:00:00.05Z'`\n* Strings that describe timestamp in [RFC2822](https://www.ietf.org/rfc/rfc2822.txt) format e.g. `'Sun, 23 Jan 2000 01:23:45 JST'`&#x20;\n\n### Safety of Credentials\n\nIn the above example, the credentials are defined in the code itself, which usually is not a good practice from a security point of view. Instead, Fennel recommends two ways of using Sources securely:\n\n1. Using environment variables\n2. Defining credentials in Fennel's web console\n\n**Using Environment Variables**\n\nHere is an example of how that may look like (this assumes you have injected appropriate secrets as env variables on your servers)\n\n```python\npostgres = Postgres(\n    name=os.getenv('POSTGRES_NAME'),\n    host=os.getenv('POSTGRES_HOST'),\n    ..\n)\n\n@source(postgres.table('user'), ..)\n@dataset\nclass UserLocation:\n    uid: int\n    ...\n```\n\n\n\n**Defining credentials in Fennel's Web Console**\n\nFennel ships with a web console that can be used for defining source credentials along with many other diagnostic and monitoring purposes. Please ask your contact at Fennel to give you access to the console if you don't yet have it.&#x20;\n\n\n\nNote that in either method, once the credentials reach the Fennel servers, they are securely stored in a cloud native Secret Manager.&#x20;\n\n### Load Impact of Sources\n\nAll sources are built in a way that they have minimal load impact on the external data sources. For instance in the above example, as long as indices are put on the cursor field, Fennel will make a single SELECT query on Postgres every minute. Besides that, there will be no other communication with it. In other words, once data is sourced into Fennel datasets, all subsequent operations are done using the copy of the data stored on Fennel servers, not the underlying data sources.\n\n### Data Copies\n\nFeature engineering systems often generate heavy read/write traffic, which often results in teams having to proactively manage capacity and/or manage caching of their data sources in lockstep with feature engineering needs. This creates operational burden and is error-prone.&#x20;\n\nFennel's design goal is to be extremely easy to install and use. To that end, Fennel has made an intentional choice to make a copy of the data from external data sources into storage inside Fennel. The benefit of this is that Fennel has negligible load impact on your data sources and hence they don't need to be scaled up / or kept in sync with load generated by feature engineering workloads. The obvious downside of this is that there are two copies of data in the system which incurs some incremental cost. But it is not as bad as it sounds -- disk storage is by far the cheapest hardware resource so it ends up being relatively cheap compared to other system components like CPU or RAM.\n\nIf this is a major deal breaker for you for some reason, please get in touch with the Fennel team - we'd love to understand your situation and try to find a workaround.&#x20;\n\n### Change Data Capture (CDC)\n\nFennel currently does not support CDC though adding CDC for at least Postgres and MySQL is on the roadmap. If you have use cases where you urgently need CDC to work, please get in touch with the Fennel team and we'd be happy to prioritize this on our roadmap.\n\n### Full List of Sources\n\nLook at [Sources](../api-reference/sources-wip.md) for a full list of sources supported by Fennel or let us know (over email or slack) if you want to use a data source that is not yet supported.&#x20;\n"},"datasets/lookups":{"title":"Lookups","slug":"lookups","path":"/Users/lukesmetham/Code/fennel/turbo/apps/docs/.content/md/api-reference/datasets/pipelines/README.md","content":"---\ndescription: The bridge between read and write paths\n---\n\n# Lookups\n\nAs described in [Concepts Overview](../../../overview/concepts/), Datasets updates are continuously computed and stored on the write path as new rows become available. But for all this data to be useful in feature engineering, feature extractors, which run on the read path, need to be able to read the data. Some sort of bridge is needed between the read path and the write path.&#x20;\n\nThat is where dataset lookups come in. The `lookup` function, as the name specifies, is used to lookup a dataset row for the given value of all the key fields. Let's look at an example:\n\n```python\n@dataset\nclass User:\n    uid: int = field(key=True)\n    home_city: str\n    cur_city: str\n    ...\n\n@featureset\nclass UserFeature:\n    uid: int = feature(id=1)\n    name: str = feature(id=2)\n    in_home_city: bool = feature(id=3)\n    ...\n    \n    @extractor\n    def func(ts: Series[datetime], uid: Series[uid]) -> Series[in_home_city]:\n        df, found = User.lookup(ts, uid=uid) \n        return df['home_city'] == df['cur_city']\n```\n\nIn this example, we have a dataset that knows the current city of the user and their home city. And we want to write a feature that checks if the user is currently in their home city or not. To do this, the extractor for the feature looks up the dataset in line 18. Note that the lookup method is directly called on the `User` class, not on any instance object. And the method takes two arguments in this case - let's look at both of them:\n\n* The first one is a positional argument that describing a series of timestamps at which lookups happen. This argument will almost always be passed as it is from extractor signature to lookup and is set at the very top entry point depending on whether the extraction is online or for historical features.&#x20;\n* The second one is a kwarg called `uid.`This expects a series of data that will be matched against `uid` field of User since that is a key field. If there were more key fields in the User dataset, the lookup method would have expected more kwargs, one for each key field of the dataset.\n\nMore specifically, the signature of the lookup method for a dataset`ds` with n key fields, has the following signature:\n\n```python\ndef lookup(\n    self, \n    ts: Series[datetime],  \n    <ds_key1>: Series[<key1_dtype], \n    <ds_key2>: Series[<key2_dtype],\n    ... ,\n    <ds_keyn>: Series[<key1_dtype], \n    fields: List[str],\n) -> Tuple[Dataframe, Series[bool]]\n```\n\nIf you wanted to read only a few fields from the dataset, it's possible to specify that via the `fields` arguments in line 8.\n\nLookup method, like all other functional interfaces of Fennel, is batched to speed up cases when lots of data needs to be read at once. You're welcome to create a series with a single element and pass that if you want to read only one element.\n\n### Return Types\n\nThe lookup method always returns a dataframe with the requested data and a series of booleans which denotes if the data was found or not. For instance, if there is no data in the dataset corresponding to the ith key, the ith element of this series will be `False`.&#x20;\n\nThe corresponding values in dataframe will be set to `None`. It is strongly advised (unlike this example), to explicitly check / handle the case when some data is not found. A common approach is to use `fillna` function of Pandas to substitute Nones with valid default values.\n\n<Hint type=\"info\">Under the hood, lookup calls for online extraction and historical extraction hit different codepaths. The former is optimized for latency and the latter for throughput.</Hint>\n\n\n\n****\n"},"datasets/pipelines":{"title":"Pipelines","slug":"pipelines","path":"/Users/lukesmetham/Code/fennel/turbo/apps/docs/.content/md/datasets/pipelines.md","content":"# Pipelines\n\nOnce you have defined datasets and sourced them them from external datasets, you might want to derive new datasets from existing datasets. That is where pipelines come in! Pipelines are functions defined on a dataset that describe how a Dataset can be derived from one or more existing Datasets. Let's look at a quick example:\n\n### Example\n\nImagine we have the following datasets defined in the system:\n\n```python\n@dataset\nclass User:\n    uid: int\n    dob: datetime    \n    country: str\n    signup_time: datetime\n\n@dataset\nclass Transaction:\n    uid: int\n    amount: float\n    payment_country: str\n    merchant_id: int\n    timestamp: datetime\n```\n\nAnd we want to create a dataset which represents some stats about the transactions made by a user in a country different from their home country. We'd write that dataset as follows:\n\n```python\n@dataset\nclass UserTransactionsAbroad:\n    uid: int = field(key=True)\n    count: int\n    amount_1d: float\n    amount_1w: float\n    \n    @pipeline(User, Transaction)\n    @classmethod\n    def first_pipeline(cls, user: Dataset, transaction: Dataset):\n        joined = transaction.join(user, on=['uid'])\n        abroad = joined.filter(lambda df: df['country'] != df['payment_country'])\n        return abroad.groupby(['uid']).aggregate(\n            Count(window='forever', into_field='count'),\n            Sum(of='amount', window='1d', into_field='amount_1d'),\n            Sum(of='amount', window='1d', into_field='amount_1w'),\n        )\n```\n\nThere is a lot happening here so let's break it down line by line:\n\nLines 1-6 are defining a regular dataset - hopefully you are comfortable with these by now. Just note that this dataset has the schema that we desire to create.\n\nLines 10-17 describe the actual pipeline code - we'd come to that in a second. Line 9 declares that this is a classmethod - all pipelines are classmethods. `pipeline` decorator itself wraps `classmethod` decorator so you can omit `classmethod` in practice - here it is shown for just describing the concept. And line 8 declares that the following code represents a pipeline - all pipelines are decorated with `pipeline` decorator. This decorator takes the names of one or more existing datasets - these are the base datasets using which new datasets will be derived. In this case, this pipeline is declaring that it starts from `User` and `Transaction` datasets.\n\nNow if you carefully inspect the signature of the pipeline function in line 10, you'd notice that it takes 2 arguments besides `cls` - they are essentially symbols for `User` dataset and `Transaction` dataset respectively.&#x20;\n\nThe function body is able to do manipulate these symbols and create other dataset objects. For instance, line 11 joins these two datasets and the resulting dataset is stored in variable `joined`. Line 12 does a `filter` operation on `joined` and stores the result in another dataset called `abroad`. Finally lines 13-17 aggregate the `abroad` dataset and create a dataset matching the schema defined in lines 3-6.&#x20;\n\nThat's your first pipeline in broad strokes but there is still lot more to unpack. Let's dive deep to get a deeper understanding of what is actually happening here.&#x20;\n\n### Interplay with Pandas Dataframes\n\nYou might have noticed that the line 12 in the above example takes a lambda with a parameter called `df` - if you guessed it to be a pandas dataframe, you guessed it exactly right!&#x20;\n\nFennel pipeline topology is defined via a handful of operators like `filter`, `transform`, `join` etc. and some of these operators let you specify the actual logic using Python functions. For these Python functions, all input and output variables are Pandas Dataframe or Pandas Series. Here is an example with `transform` operator demonstrating this:\n\n```python\n@dataset\nclass SomeDataset:\n    <some fields>\n    ...\n\n    @pipeline(Activity, UserInfoDataset)\n    def create_fraud_dataset(cls, activity: Dataset, user_info: Dataset):\n\n        def extract_info(df: pd.DataFrame) -> pd.DataFrame:\n            df[\"meta_dict\"] = df[\"metadata\"].apply(json.loads).apply(pd.Series)\n            df[\"transaction_amount\"] = 1 + df[\"metadata_dict\"]\n            return df[[\n                    \"merchant_id\",\n                    \"transaction_amount\",\n                    \"user_id\",\n                    \"timestamp\",\n                ]]\n\n        filtered_ds = activity.filter(lambda df: df[df[\"type\"] == \"report_txn\"])\n        ds = filtered_ds.join(user_info, on=[\"user_id\"])\n        return ds.transform(extract_info)\n```\n\nHere is line 21, the pipeline is running a transform operator and is passing a regular Python function to that (which is defined in line 9-17). The input and output of that function are both Pandas dataframes and within the body of that function, you can carry out arbitrary Pandas operations. This interplay with Pandas allows you to define very complex pipelines in familiar libraries instead of having to run a new DSL.\n\n### **How do the pipelines actually work?**\n\nWhen a sync call is made, Fennel client parses all the pipelines on all the datasets and runs those functions right there (which is possible since they are classmethods with no state) - the output of the pipeline function is interpreted as an AST describing the pipeline topology. This pipeline topology is sent to Fennel servers where the server type-checks the pipeline nodes and materializes a herd of jobs. Each such job is a continuous event loop waiting for new data to arrive before doing their computation and forwarding it to other jobs until the data reaches the destination dataset.&#x20;\n\n### Execution: Streaming ~~Vs~~ And Batch\n\nThe pipelines declaratively specify what should happen with the data without getting into the weeds of how it will run, how it will be partitioned/scale etc. Fennel \"compiles\" the pipeline and materializes various \"jobs\" that are needed for the pipeline to work and manages their state/scaling etc automatically.&#x20;\n\nOne of the most powerful aspects of pipelines is that the same pipeline definition will work no matter what is the source of the datasets. In the above example, for instance, `User` dataset could come from a batch source and `Transaction` dataset could come from say a streaming Kafka source and it will work exactly the same way.\n\nIn more technical terms, Fennel is built on top of [Kappa architecture](https://www.kai-waehner.de/blog/2021/09/23/real-time-kappa-architecture-mainstream-replacing-batch-lambda/) and models both the realtime and the batch cases as streaming computation.&#x20;\n\n### Operators\n\nFennel supports a handful of very general purpose operators which together form the building blocks of any pipeline. You can read about all the operators [here](pipelines.md#operators). Further, transform operator takes free-form Python using which arbitrary computation can be done (including making calls into external services if needed).\n\nThe ONLY constraint on the pipeline topology is that `aggregate` has to be the terminal node i.e. it's not allowed to compose any other operator on the output of `aggregate` operator. This very limited constraint allows Fennel to significantly reduce costs/perf of pipelines. And it's possible that even this constraint will be removed in the future.&#x20;\n\n### Schema Propagation Via Pipelines\n\nWhenever a new pipeline is first synced with the server, Fennel inspects schemas of all the datasets and verifies that they are mutually compatible all the way from input datasets to the destination datasets (with the exception of transform operator body - see [here](pipelines.md#operators) for details). As a result, Fennel is able to catch any schema mismatch errors at sync time itself.\n\n### **Nested Pipelines**\n\nIt's completely valid to write pipelines where the input datasets themselves have a pipeline in their definition. For instance, imagine we have four datasets - D1, D2, D3, and D4. D1 is somehow sourced from an external dataset. D2 is derived from D1 via a pipeline. And both D3 and D4 are derived from D2 via their own pipelines - this is valid and normal. In fact, this pattern can be used to reduce costs by computing the intermediate datasets only once. In this example, for instance, D2 is created only once and reused in both D3 and D4.&#x20;\n\n### **Multiple pipelines**\n\nIt is valid to have a dataset with multiple pipelines - in such a case, all the pipelines are independently run and the destination dataset is a union of all their outputs.&#x20;\n\nHere is an example:\n\n```python\n@dataset\nclass AndroidLogins:\n    uid: int\n    login_time: datetime\n\n@dataset\nclass IOSLogins:\n    uid: int\n    login_time: datetime\n\n@dataset\nclass LoginStats:\n    uid: int = field(key=True)\n    platform: str = field(key=True)\n    num_logins_1d: int\n\n    @pipeline(AndroidLogins)\n    def android_logins(cls, logins: Dataset) -> Dataset:\n        aggregated = logins.groupby(['uid']).aggregate([\n            Count(window=Window('1d'), into_field='num_logins_1d'),\n        ])\n        return aggregated.transform(lambda df: add_platform(df, 'android')\n\n    @pipeline(IOSLogins)\n    def ios_logins(cls, logins: Dataset) -> Dataset:\n        aggregated = logins.groupby(['uid']).aggregate([\n            Count(window=Window('1d'), into_field='num_logins_1d'),\n        ])\n        return aggregated.transform(lambda df: add_platform(df, 'ios')    \n\ndef add_platform(df: pd.Dataframe, name: str) -> pd.Dataframe:\n    df['platform'] = name\n    return df\n                            \n```\n\nHere imagine that we have two different datasets, potentially with their own separate external sources - corresponding to login activity on Android and iOS devices. And we want to create a dataset that \"merges\" rows from both, just tagged with the platform name. That can be done by having a dataset with two pipelines - in this example `android_logins` and `ios_logins`.&#x20;\n\n### **Implementing Lambda Architecture Via Multiple Pipelines**\n\nGenerally speaking, Fennel itself follows [Kappa architecture](https://www.kai-waehner.de/blog/2021/09/23/real-time-kappa-architecture-mainstream-replacing-batch-lambda/) and expresses all computation via streaming. But it can be trivially used to implement [lambda architecture](https://www.databricks.com/glossary/lambda-architecture) for your usecases.&#x20;\n\nImagine you have a realtime source (say in Kafka) and a batch source (say in Snowflake) that is batch corrected every night. And you'd like to do some computation using Kafka in realtime but also correct the data later when Snowflake data is available. That can be trivially done by writing a dataset having two pipelines - one of them can build on top of Kafka and work realtime. The other one can build on Snowflake and process the same data later.&#x20;\n\nIn this way, batch corrected data is put in the destination dataset later, and hence ends up \"overwriting\" the earlier realtime data from Kafka pipeline, hence giving you the full power of lambda architecture.&#x20;\n\n****\n"},"datasets/operators":{"title":"Operators","slug":"operators","path":"/Users/lukesmetham/Code/fennel/turbo/apps/docs/.content/md/datasets/operators.md","content":"# Operators\n\nFennel supports only the following operators, which can be composed in to express complex pipelines.\n\n1. Filter\n2. Transform\n3. Join\n4. Group By\n5. Aggregate\n6. Explode \\[Coming Soon]\n\nLet's dive deep in the concepts behind these. Meanwhile, you can look up detailed API specs for each of these [here](../api-reference/operators-wip.md).\n\n### Filter\n\nFilter is used to filter some rows from a dataset. It takes a Python function with a single argument - a Pandas Dataframe and returns a boolean series of the same size as the number of rows in the dataframe. All indices where the returned value has True are kept and all others are discarded. Here is an example:\n\n```python\n@dataset\nclass Action:\n    uid: int\n    action_type: str\n    timestamp: datetime\n\n@dataset\nclass Likes:\n    uid: int\n    timestamp: datetime\n    \n    @pipeline(Action)\n    def filter_likes(cls, actions):\n        return actions.filter(lambda df: df['action_type'] == 'like')    \n```\n\n### Transform\n\nTransform is used to add, remove, or rename columns and takes a Python function with a single argument - a Pandas dataframe and returns back the resulting dataframe. To be able to track schemas, transform also needs to describe the expected schema of the output dataset. The only restriction on transform is that it can not modify key or timestamp columns - it can only add, remove, or rename non-key, non-timestamp columns (also called as 'value columns' within Fennel).&#x20;\n\nExcept for this constraint, it can do anything, including but not limited to hitting an API endpoint and enriching the dataframe with a column from that information. Here is an example:\n\n```python\n@dataset\nclass Rating:\n    movie: str = field(key=True)\n    rating: float\n    timestamp: datetime\n\n@dataset\nclass RatingRescaled:\n    movie: str = field(key=True)\n    rescaled: float\n    timestamp: datetime\n\n    @pipeline(Rating)\n    def pipeline_transform(cls, ratings: Dataset):\n        def rescale(df: pd.DataFrame) -> pd.DataFrame:\n            df['rescaled'] = df['rating']  / 5\n            return df[['movie', 'timestamp', 'rescaled']]\n\n        return ratings.transform(rescale, schema={\n            'movie': str,\n            'timestamp': datetime,\n            'rescaled': float,\n        })\n```\n\nIn this example, the transform operator in line 20 takes the function `rescale` and an expected output schema. Note that the key of the output dataset is still`movie` and the timestamp field is still `timestamp` even though that's not mentioned explicitly.\n\n### Join\n\nJoin operator, as the name implies joins two datasets. It takes a list of field names that should be used for joining. Here is an example:\n\n```python\n@dataset\nclass Product:\n    pid: int = field(key=True)\n    seller_id: int\n    creation: datetime\n\n@dataset\nclass OrderActivity:\n    uid: int\n    pid: int\n    at: datetime\n\n@dataset\nclass UserSellerActivity:\n    uid: int\n    seller_id: Option[int]\n    at: datetime                          \n    \n    @pipeline(Product, OrderActivity):\n    def join_orders(cls, products: Dataset, orders: Dataset) -> Dataset:\n        return orders.join(products, on=['pid'])     \n```\n\nHere are some more notes and constraints on join:\n\n* The right side dataset must be a named dataset, not an intermediate dataset\n* The join can only be done on the keys of the right side i.e. right side dataset must have keys and the only join condition is equality with the keys of the right side\n* The joined dataset has the same keys & timestamp fields as the left side (and it's okay if left side itself had no keys - in that case, joined dataset also doesn't have keys)\n* Technically, semantics of this operator are that of `leftjoin` i.e. if there is no row corresponding to `pid` in the `Product` dataset, a row will still be emitted, it will just have `None` in the place of `seller_id.`As a result, the data types of new columns need to be optional. That is why `seller_id` in line 16 is `Option[int]` even though `seller_id` in `Product` dataset itself is just `int`\n\n### Group By & Aggregate\n\nGroupby and aggregate operators are used together to aggregate a dataset. Groupby selects the dimensions on which grouping should happen - and these become the key fields in the resulting dataset, and aggregate operator takes a bunch of aggregation definitions. Let's look at an example:\n\n```python\nclass AdClickStream:\n    uid: int\n    adid: int\n    at: datetime\n\n@dataset\nclass UserAdStats:\n    uid: int = field(key=True)\n    num_clicks: int\n    num_clicks_1w: int\n\n    @pipeline(AdClickStream)\n    def aggregate_ad_clicks(cls, ad_clicks: Dataset) -> Dataset:\n        return ad_clicks.groupby(['uid']).aggregate(\n            Count(window=Window('forever'), into_field='num_clicks'),\n            Count(window=Window('1w'), into_field='num_clicks_1w'),\n        )\n```\n\nHere line 14 first groups the ad click stream by user ids and then counts the number of clicks done ever as well as number of clicks done in a rolling window of 1 week. Since rolling window is continuously moving forward, the value of this cell can change continuously. This is an example where Fennel's ability to track data mutations with time come into play - lookup operations as of time t on `UserAdStats` will return the number of clicks in a 1 week rolling window ending at time t (with a small approximation to keep the computation tractable).&#x20;\n\nCurrently, the following aggregation types are supported:\n\n* **Count**: counts the number of events\n* **Sum**: sums up the values of a particular column\n* **Avg**: calculating running average of a column\n\nFollowing more aggregation types will be added soon:\n\n* Min\n* Max\n* Last K\n* Decay counts\n\n### Explode \\[Coming Soon]\n\nExplode operator will work very similar to how it works in [Pandas](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.explode.html) and make it possible to convert a field of type `list[T]` into many rows with field type replaced with `T.`&#x20;\n"},"datasets/on-demand":{"title":"On Demand","slug":"on-demand","path":"/Users/lukesmetham/Code/fennel/turbo/apps/docs/.content/md/api-reference/datasets/README.md","content":"# On Demand\n\n_NOTE: on demand functionality is still being developed and will be released in March 2023._\n\nDatasets typically receive data from either an external source, or a pipeline, or both.&#x20;\n\nHowever, sometimes fetching data can either be monetarily expensive (such as fetching the credit score from an external API vendor that charges per fetch) or computationally expensive (such as computing the BERT embedding using a heavily parameterized model). For such cases, moving the computation in a regular Pipeline may be infeasible. The alternative is to move them on the read path inside feature extractors. But in that case, the computed results are not \"cached\" and have to be rerun each time.&#x20;\n\nThe ideal in such cases will be to run the computation on the read path on demand but then to cache it for future invocations. And this is a common requirement while dealing with external API based data. This is what Fennel's \"on demand\" data comes in! Let's look at an example real quick:\n\n```python\n@dataset\nclass UserCreditScore:\n    uid: int = field(key=True)\n    credit_score: int\n    \n    @on_demand(expires_after='7d')\n    def pull_from_api(ts: Series[datetime], uids: Series[uid]) -> DataFrame:\n        user_list = uids.tolist()\n        resp = requests.get(\n            API_ENDPOINT_URL, json={\"users\": user_list}\n        )\n        ....\n        return df, found\n```\n\nIn this example, a function decorated with the decorator `on_demand` was added to the dataset. In broad strokes, on the online read path, when a lookup is issued, if the data doesn't exist in the dataset, the `on_demand` function is invoked with the missing keys. The function is free to run any code and the output of the function is stored in the dataset (as cache for future invocations) and returned back as the result of the lookup call.\n\nThe `on_demand` function takes an argument `expires_after` - this specifies the duration for which the data obtained from on demand function can be used before invoking the function with the same key again.&#x20;\n\n### Signature\n\nThe signature of `on_demand` function is same as that of `lookup` but with just one difference - it's not possible to specify the subset of fields to be read. On demand must return all the fields (after all they need to be stored in the dataset)\n\n```python\ndef <function_name>(\n    cls, \n    ts: Series[datetime],  \n    <ds_key1>: Series[<key1_dtype], \n    <ds_key2>: Series[<key2_dtype],\n    ..., \n    <ds_keyn>: Series[<key1_dtype]\n): -> Tuple[Dataframe, Series[bool]]\n```\n\nCouple of notes about the semantics of on demand datasets:\n\n* On demand is only called on the online extraction path. It is never called during historical extraction (which can be looking up millions of things) or the write path.\n* Once `on_demand` is called and some valid data is found, it is inserted in the dataset for future lookups. Any downstream datasets that depend on this dataset via pipelines will also get a chance to derive new diffs for themselves on the basis of pulled data.\n\n\n\n<Content-ref url=\"pipelines/\">[pipelines](pipelines/)</Content-ref>\n\n"},"featuresets/overview-wip":{"title":"Overview [WIP]","slug":"overview-wip","path":"/Users/lukesmetham/Code/fennel/turbo/apps/docs/.content/md/featuresets/overview-wip.md","content":"# Overview \\[WIP]\n\nFeaturesets refer to a group of logically related features where each feature is backed by a Python function that knows how to extract it. A featureset is written as a Python class annotated with `@featureset` decorator. A single application will typically have many featuresets.&#x20;\n\nLet's look at an example to see how it looks:\n\n### Example\n\n```python\n@featureset\nclass Movie:\n    duration: int = feature(id=1)\n    over_2hrs: bool = feature(id=2)\n        \n    @extractor\n    def my_extractor(cls, ts: Series[datetime], durations: Series[duration]) -> Series[over_2hrs]:\n        return durations > 2 * 3600\n```\n\nAbove example defines a featureset called `Movie` with two features - `duration`, `over_2hrs`. Each feature has a [type](../api-reference/data-types.md) and is given a monotonically increasing `id` that is unique within the featureset. This featureset has one extractor - `my_extractor` that when given the `duration` feature, knows how to extract the `over_2hrs` feature. There is no extractor provided for `duration` feature - and that's okay. Every feature in a featureset can have either zero or one extractor.&#x20;\n\n### Features\n\nA featureset can have many features. Each feature is given an explicit type and an `id` that is unique within that featureset. Each feature can have zero or exactly one extractor function that is responsible for extracting the code. Features with zero extractors can not be extracted by Fennel and hence need to be provided as an `input` to the extraction process.&#x20;\n\n### Extractors\n\nExtractors are Python function that belong to a specific featureset and know how to extract one or more features of that featureset. Extractor functions are described by adding an `@extractor` decorator to the function code. Let's look at an example:\n\n```python\nfrom fennel.featuresets import extractor, depends_on\nfrom fennel.lib.schema import Series, DataFrame\nfrom geo_lib import get_geo_id\nimport pycountry\n\ndef get_country_geoid(country: str) -> Tuple[int, int]:\n    return get_geo_id(pycountry.countries[country])\n        \n@extractor\n@depends_on(UserInfoDataset)\ndef get_country_geoid(\n    ts: pd.Series, user_id: Series[userid]\n) -> Series[country_geoid]:\n    df = UserInfoDataset.lookup(ts, user_id=user_id)\n    return df[\"country\"].apply(get_country_geoid)\n```\n\n**Extractor Signature**\n\nLet's examine the signature of `func` more carefully - a lot of information is embedded in it. It takes two arguments - `ts` and `durations`. `ts` is of type `Series[datetime]`which is just the typing info for a Pandas series of timestamps (we will come to it in a minute). `durations` is of type `Series[duration]`which refers to a Pandas series of values of the feature `duration`. And this function returns a `Series[over_2hrs]` which refers to a Pandas series of values of the feature `over_2hrs`.\n\n**Input Parameters**\n\n1. Every extractor takes `ts: Series`, as the first positional argument. The `ts` parameter is used when doing a lookup operation on a Dataset. This parameter must be passed even if the extractor does not do any Dataset lookup.\n2. One or more parameters that are of the form:\n   * `Series[<feature>]`\n   * `DataFrame[<featureset>]`\n\nSeries and DataFrame are pure syntactic sugar over `pd.Series` and `pd.DataFrame` and enable users to specify the exact feature/Featureset that the extractor expects to receive as input parameters, with the actual data type.&#x20;\n\n**Return Annotation**\n\nAn extractor returns a `Series[<feature_name>]` if it is responsible for resolving a single feature or `Dataframe[<feature_1, feature_2, ..., feature_n>]` if it is responsible for resolving multiple features.&#x20;\n\nIf the extractor is resolving the entire Featureset, then return annotation is optional and can be left empty.&#x20;\n\nEach feature in Fennel effectively becomes its own type (which can help prevent lots of bugs). With this context, the signature of extractor can now be seen as reading some features&#x20;\n\n\n\n<Hint type=\"info\">Although a single extractor can extract one or more features (including the entire featureset) every feature must have at most one extractor.&#x20;</Hint>\n\n### Extracting Multiple Features\n\nIn the examples seen so far, extractors only extracted a single feature each. But it is possible for extractors to extract multiple features too. This is useful when multiple features are related and share lots of computation. Here is an example:\n\n```\n// Some code\n```\n\n\n\n### Extractors Depending on Multiple Featuresets\n\nIn the examples seen so far, extractors depended only on the features of their own featureset. However, it is possible for extractors to depend on features from other featuresets too as inputs. This is actually very common - it's common for lots of features across many featuresets to depend on more foundational features e.g `uid` or `request_time.` Here is an example:\n\n```\n// Some code\n```\n\n\n\n"},"featuresets/end-to-end-extraction":{"title":"End to End Extraction","slug":"end-to-end-extraction","path":"/Users/lukesmetham/Code/fennel/turbo/apps/docs/.content/md/featuresets/end-to-end-extraction.md","content":"# End to End Extraction\n\nOnce one or more featuresets have been defined, you can ask Fennel to extract some features (possibly belonging to many different featuresets). Let's say we have the following featuresets:\n\n```python\n@featureset\nclass User:\n    id: int = feature(id=1)\n    age: float = feature(id=2)\n    ...\n\n\n@featureset\nclass UserPost:\n    uid: int = feature(id=1)\n    pid: int = feature(id=2)\n    score: float = feature(id=3)\n    affinity: float = feature(id=4)\n    ...\n\n@featureset\nclass Request:\n    ip: str = feature(id=1)\n    ...\n\n                                                        \n```\n\nA request can be made to the Fennel servers by running the following code:\n\n```python\nfrom fennel.client import Client\n\nclient = Client(<SERVER URL>)\n\nfeature_df = client.extract_features(\n     output_feature_list=[\n         User.age,\n         UserPost.score,\n         UserPost.affinity\n         ...\n         # there are 10 features in this list\n    ],\n    input_feature_list=[\n         User.id,\n         UserPost.uid,\n         UserPost.pid,\n         Request.ip,\n    ],\n    input_df=pd.DataFrame({\n          \"User.id\": [18232, 18234],\n          \"UserPost.uid\": [18232, 18234],\n          \"UserPost.pid\": [32341, 52315],\n          \"Request.ip\": ['1.1.1.1', '2.2.2.2'],\n     }),\n)\nassert feature_df.shape == (2, 10)\n\n```\n\nA request is made to the server between lines 5-24. Lines 6-11 specify the list of features that need to be extracted - note that this contains features across different featuresets. Between lines 13-17, a list of known features are provided and the values of these features are provided in lines 19-24. Fennel will start with the output features, find their extractors, find the inputs of those extractors and continue that process recursively until it can find a path from the given input features to all the desired output features. If no such path can be found, an error is thrown.&#x20;\n\nA few notes here:\n\n* Both the output feature list and the input feature list can span multiple featuresets\n* Multiple 'rows' can be provided in the `input_df` i.e. features for all these data points are extracted together. This is a common requirement for ranking use cases where multiple candidates need to be ranked against each other.\n* Here we provided both `User.id` and `UserPost.uid` as inputs. If the semantics are such that they refer to the same user, it's possible to write an extractor, say in `UserPost` featureset depending on `User.id` that just returns the input back. If that was done, you could get away by providing only `User.id`. More generally, this way, featuresets can be linked such that final extraction calls only require primitive IDs and maybe some context from the request.\n"},"featuresets/reading-datasets":{"title":"Reading Datasets","slug":"reading-datasets","path":"/Users/lukesmetham/Code/fennel/turbo/apps/docs/.content/md/featuresets/reading-datasets.md","content":"# Reading Datasets\n\nMore often than not, features need to be built upon some data precomputed on the write path in a Dataset. Extractors need to be able to read datasets. As explained in the dataset lookup section, lookups are the bridge between datasets and featuresets. Here is an example of how that looks:\n\n```python\nfrom fennel.featuresets import depends_on, featureset, extractor, feature\n\n@dataset\nclass User:\n    uid: int = field(key=True)\n    name: str\n    ..\n\n@featureset\nclass UserFeatures:\n    uid: int = feature(id=1)\n    name: str = feature(id=2)\n    ..\n    \n    @extractor\n    @depends_on(User)\n    def func(cls, ts: Series[datetime], uids: Series[uid]) -> Series[name]:\n        names, found = User.lookup(ts, uid=uids)\n        names.fillna('unknown')\n        return names\n                        \n```\n\nHere, in line 18, the extractor is able to read the names of the users from the dataset. But since this creates a dependency between the featureset and the dataset, Fennel requires the dependency to be declared explicitly. In line 16, the extractor specifies that it is going to do lookups on `User` dataset. If an extractor does lookups on many datasets, all their names can be passed as a list e.g. `@depends_on(User, Post, SomeDataset)`\n"},"featuresets/request-based-features":{"title":"Request Based Features","slug":"request-based-features","path":"/Users/lukesmetham/Code/fennel/turbo/apps/docs/.content/md/featuresets/request-based-features.md","content":"# Request Based Features\n\nMany ML features in realtime applications like recommendations or fraud detection depend on the context of the user request. Such features and their dependencies are very naturally modeled in Fennel. Let's look at one good way of doing this:\n\n```python\n@featureset\nclass SearchRequest:\n    uid: int = feature(id=1)\n    time: datetime = feature(id=2)\n    ip: str = feature(id=3)\n    device_type: str = feature(id=4)\n    query: str = feature(id=5)\n\n@featureset\nclass UserFeatures:\n    uid: int = feature(id=1)\n    ...\n    ctr_by_device_type: float = feature(id=17)\n    ..\n    \n    @extractor\n    fn f(cls, ts: Series[datetime], devices: Series[SearchRequest.device_type]): \n        for device in devices:\n            ...\n\n```\n\nIn this example, we defined a featureset called `SearchRequest` that contains all the properties of the request that are relevant for a feature. Features of other featuresets can now depend on `SearchRequest` features - in this example some features of `UserFeatures` are depending on `SearchRequest.device_type.` This way, as long as `SearchRequest` features are passed as input to extraction process, all such other features can be naturally computed.\n"},"featuresets/composite-features":{"title":"Composite Features","slug":"composite-features","path":"/Users/lukesmetham/Code/fennel/turbo/apps/docs/.content/md/featuresets/composite-features.md","content":"# Composite Features\n\nIn many cases, it's important to write features about a composite of two or more objects. For instance, in content recommendation, there are some features which may only be defined for (user, content) pair e.g. affinity of user with content creator. Even these complex features can be naturally modeled using Fennel featuresets. Here is a relatively complex example:\n\n```python\n@featureset\nclass User:\n    id: int = feature(id=1)\n    name: str = feature(id=2)\n    ..\n\n@featureset\nclass Post:\n    id: int = feature(id=1)\n    creator_uid: int = feature(id=2)    \n    ...\n    \n    @extractor\n    def creator(cls, ts: Series[datetime], pids: Series[id]) -> Series[creator_uid]:\n        <some code here>\n        ...\n\n@featureset\nclass UserCreator:\n    # describes features for (uid, creator_uid) pairs\n    viewer: int = feature(id=1)\n    creator: int = feature(id=2)\n    affinity: float = feature(id=3)\n    ...\n\n    @extractor\n    def affinity_fn(cls, ts: Series[datetime], viewers: Series[viewer], \n                    creators: Series[creator]) -> Series[affinity]:\n        <some code here>\n        ...\n\n@featureset\nclass UserPost:\n    # describes features for (uid, pid) pairs\n    uid: int = feature(id=1)\n    pid: int = feature(id=2)\n    viewer_author_affinity = feature(id=3)\n    ...\n    \n    @extractor\n    def fn(cls, ts: Series[datetime], uids: Series[uid], pids: Series[pid]):\n        creators = Post.creator(ts, pids)\n        return UserCreator.affinity_fn(ts, uids, creators)yth\n            \n```\n\nA lot is happening here. In addition to featureset for `User`, and `Post`, this example also has a couple of composite featuresets -- `UserCreator` to capture features for (uid, creator) pairs and `UserPost` for capturing features of (uid, post) pairs.&#x20;\n\nFurther, extractors can depend on extractors of other featuresets - here is line 42, the extractor first uses an extractor of `Post` featureset to get creators for the posts and then users an extractor of `UserCreator` to get the affinity between those users and creators.&#x20;\n\n\n\nThis way, it's possible to build very complex features by reusing machinery built for other existing features. And Fennel is smart enough to figure out the best way to resolve dependencies across featuresets (or throw an error if they can't be satisfied).&#x20;\n"},"featuresets/lifecycle-management":{"title":"Lifecycle Management","slug":"lifecycle-management","path":"/Users/lukesmetham/Code/fennel/turbo/apps/docs/.content/md/featuresets/lifecycle-management.md","content":"# Lifecycle Management\n\nML Features have a complex lifecycle. Here are some common scenarios:\n\n* Features are added for experimentation and need to be removed if the experiment doesn't show metric wins\n* A bug is discovered in a feature that is already live in a model. Sometimes it is desirable to patch the feature live (which may mean that model is now getting feature distribution for which it wasn't trained) and sometimes it is desirable to create a separate new feature and use it in a new training run before deprecating the older one.&#x20;\n* Sometimes a feature depends on a data pipeline. And data pipeline is removed while the feature is still live in a production model. That leads to feature getting invalid values leading to silent model degradation\n\nFennel aims to handle these and more cases in the \"right way\" and preventing users from making common bugs.\n\n### Immutability, Versioning, & Evolution\n\nIndividual features are immutable once created - i.e. their name, the `id`, the `type`, and their extractor code can never change. This is done to prevent a host of bugs and issues when feature code changes over time leading to models getting features that they were not trained on.&#x20;\n\nWhile individual features are immutable, featuresets can evolve over time by adding/removing features - this is done by simply adding new features with higher `id` and/or deprecating/deleting older features. This behavior is similar to adding/deprecating tags in protobufs. The way to \"fix\" a feature is to add a new feature with different ID and optionally mark the previous versoin as deprecated/deleted.&#x20;\n\n### Dependency Validation\n\nFennel explicitly tracks the lineage graph across ALL featuresets and datasets - as a result, Fennel is able to identify cases when something (say a feature) recursively depends on something (say a dataset) that is now being deleted. Fennel checks the integrity of dependency graph at the `sync` time and doesn't let the system get in such states.&#x20;\n\nAs a result, the only way to delete something (a dataset, a feature, whole featureset) is to first delete all other things that recursively depend on this.\n\n### Metaflags\n\nSimilar to dataset metaflags, featuresets, features, and extractors can also be annotated with [metaflags](../governance/metaflags.md) to manage their life cycle. Here is an example:\n\n```python\n@owner('anti-fraud-team@fintech.com')\n@featureset\nclass Movie:\n    duration: int = feature(id=1).meta(description='duration in seconds')\n    over_2hrs: bool = feature(id=2).meta(owner='laura@fintech.com')\n        \n    @extractor\n    def func(cls, ts: Series[datetime], durations: Series[duration]) -> Series[over_2hrs]:\n        return durations > 2 * 3600\n```\n\nHere is what each metaflag means in the context of features:\n\n1. **Owner** - the email of the person/entity who is responsible for maintaining health of the featureset/feature. Fennel requires every featureset to have an owner to encourage healthy code maintenance.&#x20;\n2. **Deprecated** - a feature or featureset can be marked as deprecated to communicate the intent of removing it at some point of time in the future. This serves as documentation and deters new dependencies on the deprecated feature. In the near term, it will be possible to configure Fennel to send periodic emails to any downstream users of the deprecated feature to nudge them to migrate to alternatives.&#x20;\n3. **Deleted** -  when a feature is marked as deleted it will no longer be available in the system. Using deleted features elsewhere in the system will fail at the sync time itself.\n4. **Description** - optional description to provide to any feature - most commonly used for documentation, not only in code but also in metadata APIs, in Fennel console etc.\n5. **Tags** - list of arbitrary string tags associated with each feature.\n"},"testing-and-ci-cd/unit-tests":{"title":"Unit Tests","slug":"unit-tests","path":"/Users/lukesmetham/Code/fennel/turbo/apps/docs/.content/md/testing-and-ci-cd/unit-tests.md","content":"# Unit Tests\n\nFennel's Python client ships with an (inefficient) mock server inside it - this makes it possible to do local development and run unit tests against the mock server to verify correctness.  This works even if you don't have any remote Fennel server - heck it works even if you don't have internet.&#x20;\n\nThis mock server has near parity with the actual server with one notable exception - it doesn't support data connectors to external data systems (after all, it is completely local with zero remote dependencies!)\n\n### Example\n\nLet's first see how it will work and later we will see a fully functional unit test example.\n\n```python\nfrom fennel.test_lib import mock_client\n\nclass TestDataset(unittest.TestCase):\n    @mock_client\n    def test_dataset(self, client):\n        # client talks to the mock server\n        # ... do any setup\n        # Sync the dataset\n        client.sync(datasets=[User])\n        # ... some other stuff\n        client.log('User', pd.Dataframe(...))\n        # ... some other stuff\n        found = client.extract_features(...)\n        self.assertEqual(found, expected)    \n```\n\nHere we imported `mock_client` from the `test_lib`. This is a decorator which can be used to decorate test functions - and the decorator supplies an extra argument called `client` to the test. Once the `client` object reaches the body of the test, you can do all operations that are typically done on a real client - you can sync datasets/featuresets, log data, extract features etc.&#x20;\n\nSince external data integration doesn't work in mock server, the only way to bring data to a dataset in the mock server is by explicitly logging data to it.\n\n\n\n## Testing Datasets\n\nFor testing Datasets, you can use the `client.log` to add some local data to a dataset and then query this or other downstream datasets using the `.lookup` API. Here is an end to end example. Suppose our regular non-test code looks like this:\n\n```python\nimport pandas as pd\nfrom fennel.datasets import dataset, field\n\n@meta(owner=\"test@test.com\")\n@dataset\nclass RatingActivity:\n    userid: int\n    rating: float\n    movie: str\n    t: datetime\n\n\n@meta(owner=\"test@test.com\")\n@dataset\nclass MovieRating:\n    name: str = field(key=True)\n    rating: float\n    num_ratings: int\n    sum_ratings: float\n    t: datetime\n\n    @staticmethod\n    @pipeline(RatingActivity)\n    def pipeline_aggregate(activity: Dataset):\n        ds = activity.groupby(\"movie\").aggregate([\n            Count(window=Window(), name=\"num_ratings\"),\n            Sum(window=Window(), value=\"rating\", name=\"sum_ratings\"),\n            Average(window=Window(), value=\"rating\", name=\"rating\"),\n        ])\n        return ds.transform(lambda df: df.rename(columns={\"movie\": \"name\"}))\n```\n\nAnd you want to test that data reaching `RatingActivity` dataset correctly propagates to `MovieRating` dataset via the pipeline. You could write the following unit test to do so:\n\n```python\nfrom fennel.test_lib import mock_client\nfrom my_dataset import MovieRating, RatingActivity\n\nclass TestDataset(unittest.TestCase):\n    @mock_client\n    def test_dataset(self, client):\n        # Sync the dataset\n        client.sync(\n            datasets=[MovieRating, RatingActivity],\n        )\n        now = datetime.now()\n        one_hour_ago = now - timedelta(hours=1)\n        two_hours_ago = now - timedelta(hours=2)\n        three_hours_ago = now - timedelta(hours=3)\n        four_hours_ago = now - timedelta(hours=4)\n        five_hours_ago = now - timedelta(hours=5)\n\n        data = [\n            [18231, 2, \"Jumanji\", five_hours_ago],\n            [18231, 3, \"Jumanji\", four_hours_ago],\n            [18231, 2, \"Jumanji\", three_hours_ago],\n            [18231, 5, \"Jumanji\", five_hours_ago],\n            [18231, 4, \"Titanic\", three_hours_ago],\n            [18231, 3, \"Titanic\", two_hours_ago],\n            [18231, 5, \"Titanic\", one_hour_ago],\n            [18231, 5, \"Titanic\", now - timedelta(minutes=1)],\n            [18231, 3, \"Titanic\", two_hours_ago],\n        ]\n        columns = [\"userid\", \"rating\", \"movie\", \"t\"]\n        df = pd.DataFrame(data, columns=columns)\n        response = client.log(\"RatingActivity\", df)\n        assert response.status_code == requests.codes.OK\n\n        # Do some lookups to verify pipeline_aggregate \n        # is working as expected\n        ts = pd.Series([now, now])\n        names = pd.Series([\"Jumanji\", \"Titanic\"])\n        df, _ = MovieRating.lookup(\n            ts,\n            movie=names,\n        )\n        assert df.shape == (2, 5)\n        assert df[\"movie\"].tolist() == [\"Jumanji\", \"Titanic\"]\n        assert df[\"rating\"].tolist() == [3, 4]\n        assert df[\"num_ratings\"].tolist() == [4, 5]\n        assert df[\"sum_ratings\"].tolist() == [12, 20]\n\n```\n\n### Testing Featuresets\n\nExtractors are simple Python functions and, hence, can be unit tested directly.\n\n```python\n# this is the non-test code\n@meta(owner=\"test@test.com\")\n@featureset\nclass UserInfoMultipleExtractor:\n    userid: int = feature(id=1)\n    name: str = feature(id=2)\n    # The users gender among male/female/non-binary\n    age: int = feature(id=4).meta(owner=\"aditya@fennel.ai\")\n    age_squared: int = feature(id=5)\n    age_cubed: int = feature(id=6)\n    is_name_common: bool = feature(id=7)\n\n    @extractor\n    def get_age_and_name_features(\n        ts: pd.Series, user_age: Series[age], name: Series[name]\n    ) -> DataFrame[age_squared, age_cubed, is_name_common]:\n        is_name_common = name.isin([\"John\", \"Mary\", \"Bob\"])\n        return pd.concat([user_age**2, user_age**3, is_name_common], axis=1)\n        \n# somewhere in the test file, you can write this        \nclass TestSimpleExtractor(unittest.TestCase):\n\tdef test_get_age_and_name_features(self):\n        age = pd.Series([32, 24])\n        name = pd.Series([\"John\", \"Rahul\"])\n        ts = pd.Series([datetime(2020, 1, 1), datetime(2020, 1, 1)])\n        df = UserInfoMultipleExtractor.get_age_and_name_features(ts, age, name)\n        self.assertEqual(df.shape, (2, 3))\n        self.assertEqual(df[\"age_squared\"].tolist(), [1024, 576])\n        self.assertEqual(df[\"age_cubed\"].tolist(), [32768, 13824])\n        self.assertEqual(df[\"is_name_common\"].tolist(), [True, False])\n```\n\n\nFor extractors that depend on dataset lookups, the setup looks similar to that of testing datasets as shown above - create a mock client, sync some datasets/featuresets, log data to a dataset, and finally use client to extract features. Here is an example:\n\n```python\n# this is regular non-test code\n@meta(owner=\"test@test.com\")\n@dataset\nclass UserInfoDataset:\n    user_id: int = field(key=True)\n    name: str\n    age: Optional[int]\n    timestamp: datetime = field(timestamp=True)\n    country: str\n\n\n@meta(owner=\"test@test.com\")\n@featureset\nclass UserInfoMultipleExtractor:\n    userid: int = feature(id=1)\n    name: str = feature(id=2)\n    country_geoid: int = feature(id=3).meta(wip=True)  # type: ignore\n    # The users gender among male/female/non-binary\n    age: int = feature(id=4).meta(owner=\"aditya@fennel.ai\")  # type: ignore\n    age_squared: int = feature(id=5)\n    age_cubed: int = feature(id=6)\n    is_name_common: bool = feature(id=7)\n\n    @extractor\n    @depends_on(UserInfoDataset)\n    def get_user_age_and_name(\n        ts: pd.Series, user_id: Series[userid]\n    ) -> DataFrame[age, name]:\n        df = UserInfoDataset.lookup(ts, user_id=user_id)  # type: ignore\n        return df[[\"age\", \"name\"]]\n\n    @extractor\n    def get_age_and_name_features(\n        ts: pd.Series, user_age: Series[age], name: Series[name]\n    ) -> DataFrame[age_squared, age_cubed, is_name_common]:\n        is_name_common = name.isin([\"John\", \"Mary\", \"Bob\"])\n        return pd.concat([user_age**2, user_age**3, is_name_common], axis=1)\n\n    @extractor\n    @depends_on(UserInfoDataset)\n    def get_country_geoid(\n        ts: pd.Series, user_id: Series[userid]\n    ) -> Series[country_geoid]:\n        df = UserInfoDataset.lookup(ts, user_id=user_id)  # type: ignore\n        return df[\"country\"].apply(get_country_geoid)\n        \n\n# this is your test code in some test module                \nclass TestExtractorDAGResolution(unittest.TestCase):\n    @mock_client\n    def test_dag_resolution(self, client):\n        client.sync(\n            datasets=[UserInfoDataset],\n            featuresets=[UserInfoMultipleExtractor],\n        )\n        data = [\n            [18232, \"John\", 32, \"USA\", 1010],\n            [18234, \"Monica\", 24, \"Chile\", 1010],\n        ]\n        columns = [\"user_id\", \"name\", \"age\", \"country\", \"timestamp\"]\n        df = pd.DataFrame(data, columns=columns)\n        response = client.log(\"UserInfoDataset\", df)\n        assert response.status_code == requests.codes.OK, response.json()\n\n        feature_df = client.extract_features(\n            output_feature_list=[\n                UserInfoMultipleExtractor,\n            ],\n            input_feature_list=[UserInfoMultipleExtractor.userid],\n            input_df=pd.DataFrame({\"userid\": [18232, 18234]}),\n            timestamps=pd.Series([1011, 1012]),\n        )\n        self.assertEqual(feature_df.shape, (2, 7))\n```\n"},"testing-and-ci-cd/integration-tests-wip":{"title":"Integration Tests [WIP]","slug":"integration-tests-wip","path":"/Users/lukesmetham/Code/fennel/turbo/apps/docs/.content/md/testing/integration-tests.md","content":"# Integration Tests \\[WIP]\n\nUnlike unit tests where testing is fully done using only the `mock_client`, integration tests are about testing a regular client (one that is connected to the `users` endpoint), and can test if the client is returning the expected output.&#x20;\n\n\n\n## WIP\n"},"testing-and-ci-cd/ci-cd-workflows":{"title":"CI/CD Workflows","slug":"ci-cd-workflows","path":"/Users/lukesmetham/Code/fennel/turbo/apps/docs/.content/md/testing-and-ci-cd/ci-cd-workflows.md","content":"# CI/CD Workflows\n\n### Philosophy\n\nFennel's core CI/CD philosophy is that deploying changes to Fennel should be _identical_ to the CI/CD process of the rest of your codebase - this way, your existing release processes should all continue to work. This is in contrast to alternatives where Fennel introduces its own rigid CI/CD process which may or may not work with your existing workflow.\n\nThis is achieved because `sync` calls are regular Python code in your regular code repo (vs a separate Fennel repo) and their behavior can be modified by changing the credentials of the server to which the client is connecting with.&#x20;\n\nHere is one recommended workflow:\n\n1. **Local Development** doesn't touch Fennel servers at all. Developers write new datasets/featuresets and verify that they work by writing unit tests (which provide a nearly complete mock of the real server except connectors to external datasets).&#x20;\n2. **Deployment to Fennel** happens in a Github actions (or any other post-merge) script. Credentials to prod Fennel servers are injected as environment variables in this script such that when `sync` call is made, the client ends up connecting with the prod servers. Doing this post-merge ensures that only reviewed code modifies the state\n\nThis is just one workflow and you can create any workflow by appropriately injecting credentials to Fennel's prod clusters in the right place.&#x20;\n\nFor instance, another option is to have 3 Fennel clusters - dev, staging, prod. Individual developers have access to `dev` cluster which they can change during local development as they want. Credentials to `staging` tier are injected when the rest of the code reaches the staging layer. And the credential of prod cluster are injected when the code is ready to be deployed to prod.&#x20;\n\n### Stateful Rollbacks\n\nSometimes changes need to be rolled back - this is a bit tricky because dataset/featureset changes create some state - sometimes explicitly (e.g. you might have logged data in a dataset already - should it be deleted at roll back) and sometimes implicitly (e.g. you might have used a particular `id` for a feature - does rollback release that id for others to reuse?). In that sense, it is somewhat similar to roll backs of database schema migrations.&#x20;\n\nConsequently, the best way to roll back is to not blindly revert the commit but rather explicitly mark things as deprecated or deleted before syncing again.&#x20;\n"},"governance/privacy-and-security":{"title":"Privacy & Security","slug":"privacy-and-security","path":"/Users/lukesmetham/Code/fennel/turbo/apps/docs/.content/md/misc/faq/privacy-and-security.md","content":"# Privacy & Security\n\n### General InfoSec\n\nFennel runs inside your VPC and thus is subject to your usual infosec policies. The customer data never leaves your cloud which eliminates many privacy/compliance vulnurabilities.\n\n### Data security\n\nFennel uses industry best-practices for data security:\n\n* All data is encrypted in transit and at rest\n* Secure strorage of user-provided secrets in secrete stores\n* All inter-server communication happens over TLS\n* Authentication and TLS for client-server requests\n\n### Privacy\n\nFor GDPR compliance, Fennel allows stored data to be physically deleted.\n"},"governance/metaflags":{"title":"Metaflags","slug":"metaflags","path":"/Users/lukesmetham/Code/fennel/turbo/apps/docs/.content/md/governance/metaflags.md","content":"# Metaflags\n\nFeatures and datasets are not static in the real world and have their own life cycle. Metaflags is a mechanism to annotate and manage the lifecycle of Fennel objects.&#x20;\n\nHere are a few scenarios which are pretty common in practice where Metaflags help:\n\n* A bug is discovered in a feature, resulting in it being rewritten. This now requires all downstream features to also be updated.&#x20;\n* Ownership of a dataset needs to be tracked so that if it is having data quality issues, the problem can be routed to an appropriate person to investigate.\n* Features and data need to be documented so that their users can easily understand what they are doing.&#x20;\n* Due to compliance reasons, all features that depend on PII data either directly or through a long list of upstream dependencies need to be audited - but for that, first all such features need to be identified.&#x20;\n\nLet's look at an example:\n\n```python\n@meta(owner='nikhil@xyz.ai', tags=['PII', 'hackathon'])\n@dataset\nclass User:\n    uid: int = field(key=True)\n    height: float = field().meta(description='in inches')\n    weight: float = field().meta(description='in lbs')\n    at: datetime\n\n@meta(owner='feed-team@xyz.ai')\n@featureset\nclass UserFeatures:\n    uid: int = feature(id=1)\n    zip: str = feature(id=2).meta(tags=['PII'])\n    bmi: float = feature(id=3).meta(owner='alan@xyz.ai')        \n    bmr: float = feature(id=4).meta(deperecated=True)\n    ..\n    \n    @meta(description='based on algorithm specified here: bit.ly/xyy123')\n    @extractor\n    def some_fn(...):\n        ...\n```\n\nFennel currently supports 5 metaflags:\n\n1. **owner** - email address of the owner of the object. The ownership flows down transitively. For instance, the owner of a featureset becomes the default owner of all the features unless it is explicitly overwritten by specifying an owner for that feature.&#x20;\n2. **description** - description of the object, used solely for documentation purposes.&#x20;\n3. **tags** - list of arbitrary string tags associated with the object. Tags flow across the lineage graph and are additive. For instance, if a dataset is tagged with tag 'PII', all other objects that read from the dataset will inherit this tag. Fennel supports searching for objects with a given tag.&#x20;\n4. **deleted** - whether the object is deleted or not. Sometimes it is desirable to delete the object but keep a marker tombstone in the codebase - that is where deleted should be used. For instance, maybe a feature is now deleted but its ID should not be reused again - it'd be a good idea to mark it as deleted and leave it like that forever (the code for its extractor can be removed)\n5. **deprecated** - same as deleted but just marks the object as to be deprecated in the near future. If an object uses a deprecated object, the owner will get periodic reminders to modify their object to not dependon the deprecated object any more.&#x20;\n\n<Hint type=\"info\">Most Fennel constructs are immutable by construction. However, it's possible to change their metaflags even when the rest of the details can not be changed.</Hint>\n"},"data-quality/data-expectations":{"title":"Data Expectations","slug":"data-expectations","path":"/Users/lukesmetham/Code/fennel/turbo/apps/docs/.content/md/data-quality/data-expectations.md","content":"# Data Expectations\n\nFennel's powerful [type system](../api-reference/data-types.md) lets you maintain data integrity by outright rejecting any data that doesn't meet the given types. However, sometimes there are situations when data expectations are more probabilistic in nature.&#x20;\n\nAs an example, you may have a field in dataset of type `Optional[str]` that denotes user city (can be None if user didn't provide their city). While this is nullable, in practice, we expect _most_ people to fill out their city. In other words, we don't want to reject Null values outright but still _track_ if fraction of null values is higher than what we expected.&#x20;\n\nFennel lets you do this by writing data expectations (and yes, this is based on Great Expectation - so you can reuse any expectations built by the community). Once expectations are specified, Fennel tracks the % of the rows that fail the expectation -- and can alert you about these failures. This can be a very powerful tool to catch a very large class of \"runtime\" quality issues.&#x20;\n\n### Example\n\n```python\n@dataset\nclass Sample:\n    passenger_count: between(int, 0, 100)\n    gender: str\n    age: between(int, 0, 100, strict_min=True)\n    mothers_age: between(int, 0, 100, strict_min=True)  \n    \n    @expectations\n    def my_function(cls):\n        return [\n             expect_column_values_to_be_between(\n                 column=str(cls.passenger_count),\n                 min_value=1,\n                 max_value=6,\n                 mostly=0.95,\n             ),\n             expect_column_values_to_be_in_set(\n                str(cls.gender),\n                [\"male\", \"female\"],\n                 mostly=0.99\n             ),\n            # Pairwise expectation\n            expect_column_pair_values_A_to_be_greater_than_B(\n                column_A=str(cls.age), \n                column_B=str(cls.mothers_age),\n                mostly=1,\n            ),\n     ]\n```\n\n### Type Restrictions vs Expectations\n\n[Type restrictions](../api-reference/data-types.md) and expectations may appear to be similar but solve very different purposes. Type Restrictions simply reject any row/data that doesn't satisfy the restriction - as a result, all data stored in Fennel datasets can be trusted to follow the type restriction rules.\n\nData expectations, on the other hand, don't reject the data - just passively track the frequency of expectation mismatch and alert if it is higher than some threshold. Tyep restrictions are a stronger check and should be preferred if no expections to the restriction are allowed.&#x20;\n"},"data-quality/lifecycle-management":{"title":"Lifecycle Management","slug":"lifecycle-management","path":"/Users/lukesmetham/Code/fennel/turbo/apps/docs/.content/md/data-quality/lifecycle-management.md","content":"# Lifecycle Management\n\nWhen it comes to data quality, prevention is better than the cure. To that end, three powerful ideas are built into the Fennel's core architecture that together prevent a whole class of bugs/issues organically without any external monitoring:\n\n* **Explicit Strong Typing** - all data has to be explicitly associated with types. Further, the type system is rather strong - e.g. nulls are't allowed unless the type is marked to be Optional. Fennel also supports much stronger [type restrictions](../api-reference/data-types.md) to express even more complex constraints (e.g. matching a regex)\n* **Immutability** - most objects (datasets, featuresets, extractors etc.) are immutable and can not be changed once created. So downstream users of any object can confidently rely on the object to be stable.\n* **Dependency Validation** - Fennel tracks the full dependency graph across all objects - datasets, featuresets, features, extractors etc. As a result, it is able to detect and prevent a whole class of common data/feature quality issues at \"compile\" time (i.e. when the `sync` call is made).&#x20;\n\nHere are some common sources of bug and how these two best practices prevent them:\n\n\n\n| Scenario                                                                                                                                                                      | How Fennel prevents it                                                                                                                     |\n| ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------ |\n| A feature recursively depends on some data pipeline which is later removed leading to feature values becoming all NULLs                                                       | The change that deleted the dataset would have been rejected in `sync` - because a non-deleted feature depends on it                       |\n| A feature recursively depends on some data but the data pipeline code changes, leading to wrong feature values                                                                | Pipelines are immutable and can not be modified                                                                                            |\n| A feature F1 depends on another feature F2. F1 is part of a model but F2 is not. F2's owner modifies it since no model is using it, causing distribution of F1 to also change | Feature dependencies are tracked explicitly and mutations aren't allowed.                                                                  |\n| A dataset contains a string column representing zip codes. But due to a bug upstream, non-zip code strings are added, which can break any pipelines depending on this dataset | Zip code can be expressed a specific type restriction using regex. Any data not confirming to it will not even be admitted to the dataset. |\n\n"},"data-quality/drift-monitoring":{"title":"Drift Monitoring","slug":"drift-monitoring","path":"/Users/lukesmetham/Code/fennel/turbo/apps/docs/.content/md/data-quality/drift-monitoring.md","content":"# Drift Monitoring\n\nThe distribution of ML features can change organically over time despite the system being correct. For instance, when the weather shifts from winter to summer, the demand for ACs rises sharply. As a result, the distribution of any features that capture demand of products is going to change too. This is not a problem in itself unless models haven't been refreshed in a while - in that case, the trained model's performance may degrade due to this drift in feature values.&#x20;\n\nFennel will track feature distributions and give you alerts when they skew meaningfully. This is currently work in progress and is expected to land in Q2 2023.\n\n"},"api-reference/client-wip":{"title":"Client [WIP]","slug":"client-wip","path":"/Users/lukesmetham/Code/fennel/turbo/apps/docs/.content/md/api-reference/client-wip.md","content":"# Client \\[WIP]\n\nFennel Client has the following methods on it:\n\n### extract\\_features\n\nGiven some input and output features, extracts the current value of all the output features given the values of the input features.\n\n**Arguments:**\n\n* `output_feature_list`: list of features (written as fully qualified name of a feature along with the featureset) that should be extracted\n* `input_feature_list` : list of features for which values are known\n* `input_df`: a pandas dataframe object that contains the values of all features in the input feature list. Each row of the dataframe can be thought of as one entity for which features are desired.\n* `log: bool` - boolean which indicates if the extracted features should also be logged (for log-and-wait approach to training data generation). Default is False\n* `workflow: str` - the name of the workflow associated with the feature extraction. Only relevant when `log` is set to True\n* `sampling_rate: float` - the rate at which feature data should be sampled before logging. Only relevant when log is set to True. The default value is 1.0\n\n**Example:**\n\n```python\nclient = Client(<URL>)\n\n@featureset\nclass UserInfo:\n    userid: int = feature(id=1)\n    ...\n\nfeature_df = client.extract_features(\n    output_feature_list=[\n        UserInfo.userid,\n        UserInfo.name,\n        UserInfo.geoid,\n        UserInfo.age,\n        UserInfo.age_squared,\n        UserInfo.age_cubed,\n        UserInfo.is_name_common,\n    ],\n    input_feature_list=[UserInfo.userid],\n    input_df=pd.DataFrame(\n        {\"UserInfo.userid\": [18232, 18234]}\n    ),\n    log=True,\n    workflow='home_page_ads',\n)\nassert feature_df.shape == (2, 7)\n```\n\n### **extract\\_historical\\_features**\n\n****\n\n### **sync**\n\n****\n\n### **log**\n\nWhile Fennel supports inbuilt connectors to external datasets, it's also possible to \"manually\" log data to Fennel datasets using `log`.\n\n**Arguments:**\n\n* `dataset_name: str` - the name of the dataset to which data needs to be logged\n* `dataframe: Dataframe` - the data that needs to be logged, expressed as a Pandas dataframe.&#x20;\n* `batch_size: int` - the size of batches in which dataframe is chunked before sending to the server. Useful when attempting to send very large batches. The default value is 1000.\n\nThis method throws an error if the schema of the dataframe (i.e. column names and types) are not compatible with the schema of the dataset.&#x20;\n\n**Example**\n\n```python\n@meta(owner='abc@xyz.ai')\n@dataset\nclass User:\n    id: int\n    gender: str\n    signup: datetime\n\n# create a client somehow\n# ...\nclient.sync(datasets=[User])\ndf = pd.Dataframe.from_dict({\n    'id': [1, 2, 3],\n    'gender': ['M', 'F', 'M'],\n    'signup': [1674793720, 1674793720, 1674793720],\n})\nclient.log('User', df)\n```\n"},"api-reference/rest-api":{"title":"REST API","slug":"rest-api","path":"/Users/lukesmetham/Code/fennel/turbo/apps/docs/.content/md/api-reference/rest-api.md","content":"# REST API\n\nAll DDL operations (i.e. definitions of datasets/featuresets) can only be done via Python client. However, other operations that don't alter the definitions but just exchange data can also be done via a REST API in addition to the Python client.\n\n### /api/v1/log\n\nUsed to log data to a dataset. It's post call with the following properties:\n\n* `dataset_name`: the name of the dataset to which data needs to be logged (as json string)\n* `payload`: a list of rows (as json) that must be logged to the dataset\n\n**Example**\n\n```\n// TODO\n```\n\n### /api/v1/extract\\_features\n\nUsed to extract a set of output features given known values of some input features. It's a POST call with the following parameters:\n\n* `input_features`: list of fully qualified names of input features\n* `output_features`: list of fully qualified names of desired output features\n* `data`: list of json strings, each representing a particular row of the dataframe of input feature values\n* `log`: boolean, true if the extracted features should also be logged to serve as future training data\n* `workflow`: string describing the name of the workflow to which extract features should be logged (only relevant when `log` is set to true)\n* `sampling_rate`: float between 0-1 describing the sampling to be done while logging the extracted features (only relevant when `log` is true)\n\n**Example**\n\n```\n// TODO\n```\n\n### /api/v1/extract\\_historical\\_features\n\nTODO\n"},"api-reference/duration":{"title":"Duration","slug":"duration","path":"/Users/lukesmetham/Code/fennel/turbo/apps/docs/.content/md/api-reference/duration.md","content":"# Duration\n\nDescribing some time duration is surprisingly common in feature engineering workflows. Fennel lets you express durations in an easy to read natural language as described below&#x20;\n\n<table><thead><tr><th>Signifier</th><th>Unit</th><th data-hidden></th></tr></thead><tbody><tr><td>y</td><td>Year</td><td></td></tr><tr><td>w</td><td>Week</td><td></td></tr><tr><td>d</td><td>Day</td><td></td></tr><tr><td>h</td><td>Hour</td><td></td></tr><tr><td>m</td><td>Minute</td><td></td></tr><tr><td>s</td><td>Second</td><td></td></tr></tbody></table>\n\nExamples:\n\n1. \"7h\" -> 7 hours\n2. \"12d\" -> 12 days\n3. \"2y\" -> 2 years\n4. \"3h 20m 4s\" -> 3 hours 20 minutes and 4 seconds\n5. \"2y 4w\" -> 2 years and 4 weeks\n\n<Hint type=\"info\">A year is not a fixed amount of time but is hardcoded to be exactly 365 days</Hint>\n\nNote that there is no shortcut for month because there is a very high degree of variance in month's duration- some months are 28 days, some are 30 days and some are 31 days. A common convention in feature engineering is to use `4 weeks` to describe a month.\n"},"api-reference/data-types":{"title":"Data Types","slug":"data-types","path":"/Users/lukesmetham/Code/fennel/turbo/apps/docs/.content/md/api-reference/data-types.md","content":"# Data Types\n\nFennel supports the following data types, expressed as native Python type hints.\n\n| `int`, `float`   | int is implemented as int64 and float is implemented as float64                                                                                                                                                                                  |\n| ---------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |\n| `bool`           | Booleans                                                                                                                                                                                                                                         |\n| `str`            | Arbitrary sequence of bytes                                                                                                                                                                                                                      |\n| `list[T]`        | List of elements of type `T`. Unlike Python lists, all elements must have the same type                                                                                                                                                          |\n| `dict[T]`        | Map from `str` to data of type `T`. Please let us know if your use cases require dict with non-string keys                                                                                                                                       |\n| `Optional[T]`    | Same as Python `Optional` - permits either `None` or values of type `T`                                                                                                                                                                          |\n| `Embedding[int]` | Denotes a list of floats of the given fixed length i.e. `Embedding[32]` describes a list of 32 floats. This is same as `list[float]` but enforces the list length which is important for dot product and other similar operations on embeddings. |\n| `datetime`       | Describes a timestamp, implemented as microseconds since Unix epoch (so minimum granularity is microseconds). Can be natively parsed from multiple formats.                                                                                      |\n\n### Type Restrictions\n\nImagine that you have a field that denotes a US zip code but stored as string. Not all strings denote valid zip codes - only those that match a particular regex do but this can be hard to encode, which can lead to incorrect data being stored.&#x20;\n\nFennel supports type restrictions -- these are additional constraints put on base types that restrict the set of valid values in some form. Here is a list of supported restrictions:\n\n| Restricted Type         | Base Type                           | Restriction                                                                                                                                        |\n| ----------------------- | ----------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------- |\n| `regex('<pattern>')`    | `str`                               | Permits only the strings matching the given regex pattern                                                                                          |\n| `between(T, low, high)` | `T` where T can be `int` or `float` | Only permits values between low and high (both inclusive). Left or right can be made exclusive by setting `min_strict` or `max_strict` to be False |\n| `oneof(T, [values...])` | `T`                                 | Only one of the given values is accepted. For the restriction to be valid, values themselves should be of type T                                   |\n\n\n\nThese restricted types act as regular types -- they can be mixed/matched to form complex composite types. For instance, the following are all valid Fennel types:\n\n* `list[regex('$[0-9]{5}$')]` - list of regexes matching US zip codes\n* `oneof(Optional[int], [None, 0, 1])` - a nullable type that only takes 0 or 1 as valid values\n\nNote: data belonging to these restricted types is still stored / transmitted (e.g. in json encoding) as a regular base type. It's just that Fennel will reject data of base type that doesn't meet the restriction.\n"},"api-reference/operators-wip":{"title":"Operators [WIP]","slug":"operators-wip","path":"/Users/lukesmetham/Code/fennel/turbo/apps/docs/.content/md/api-reference/operators-wip.md","content":"# Operators \\[WIP]\n\n### Join\n\n1. `dataset: Dataset` - positional argument, that specifies the RHS Dataset.&#x20;\n2. `on: List[str]` - kwarg that specifies the list of fields to join on.\n3. `left_on: List[str]` - optional kwarg specifying the list of fields to join on for the LHS dataset.&#x20;\n4. `right_on: List[str]` - optional kwarg specifying the list of fields to join on for the RHS dataset.&#x20;\n5. `fields: List[str]` - This is an optional kwarg that specifies which fields need to be taken from the RHS dataset during the join operation. If not specified, Fennel will take all the non-join and non-timestamp fields in the RHS dataset.\n\n<Hint type=\"info\">One must either provide the `on` parameter or both the `left_on` & `right_on` parameters. If providing `left_on` and `right_on` their lengths should be same.</Hint>\n\n<Hint type=\"info\">The `on` or `right_on` fields specified should be keys in the RHS Dataset.</Hint>\n\n\n\n### Filter\n\nTODO\n\n### Transform\n\nTODO\n\n### Groupby / Aggregate\n\nTODO\n\n### Explode\n\nTODO\n"},"api-reference/aggregations":{"title":"Aggregations","slug":"aggregations","path":"/Users/lukesmetham/Code/fennel/turbo/apps/docs/.content/md/api-reference/aggregations.md","content":"# Aggregations\n\nAggregations are provided to the \\`aggregate\\` operator and specify how the agggregation should happen. All aggregations take two common arguments:\n\n1. `window`: Window - argument that specifies the length of the duration across which Fennel needs to perform the aggregation. See how [duration](duration.md) is specified in Fennel.\n2. `into_field`: str - the name of the field in the output dataset that corresponds to this aggregation. This&#x20;\n\nBesides these common arguments, here is the rest of the API reference for all the aggregations:\n\n### 1. Count\n\nCount computes a rolling count for each group key across a window. Takes no additional arguments besides `window` and `into_field`. It returns 0 by default. Its output type is always `int`.&#x20;\n\n### 2. Sum &#x20;\n\nSum aggregate computes a rolling sum across a window for a given field in the dataset. This field is specified by the `of` parameter of type `str`. If no data is available, the default value is 0. Its output type can be `int` or `float` depending on the input type (and any other input type will fail sync validation)\n\n### 3. Average\n\nSame as \"Sum\", but instead maintains a rolling average in the given window. In addition to the `of` field, also requires a `default` value to be specified which is returned when average is queried for a window with no data points. The input types can only be `int` or `float` and the output type is always `float`.\n\n### 4. Min&#x20;\n\nSame as \"Sum\", but instead maintains a rolling minimum in the given window. Requires a `default` value to be specified. Input type can be `int` or `float` and the output type is same as the input type.\n\n### 5. Max&#x20;\n\nIdentical to \"min\", but instead maintains a rolling maximum in the given window.&#x20;\n\n### 6. LastK\n\nMaintains a list of \"items\" in the given rolling durations. If no events have been logged, returns an empty list. If input field is of type `T` the output field is of type `List[T]`\n"},"api-reference/sources-wip":{"title":"Sources [WIP]","slug":"sources-wip","path":"/Users/lukesmetham/Code/fennel/turbo/apps/docs/.content/md/api-reference/sources-wip.md","content":"# Sources \\[WIP]\n\nHere is the description of all the external sources supported by Fennel and how to use them:\n\n### **MySQL**\n\nThe following fields need to be specified:\n\n1. **`name`** - A name to identify the source. The name should be unique across all sources.\n2. **`host`** - The host name of the database.\n3. **`port`** - The port to connect to. By default it is 3303 for MySQL and 5432 for Posgres.\n4. **`db_name`** - The database name.\n5. **`username`** - The username which is used to access the database.\n6. **`password`** - The password associated with the username.\n7. **`jdbc_params`** - Additional properties to pass to the JDBC URL string when connecting to the database formatted as `key=value` pairs separated by the symbol `&`. (example: `key1=value1&key2=value2&key3=value3`).\n\n```python\nfrom fennel import sources\nfrom source import source\n\nmysql = sources.MySQL(\n    name='py_mysql_src',\n    host=\"my-favourite-mysql.us-west-2.rds.amazonaws.com\",\n    port=3306,\n    db_name=\"some_database_name\",\n    username=\"admin\",\n    password=\"password\",\n    jdbc_params=\"enabledTLSProtocols=TLSv1.2\",\n)\n\n@source(mysql.table('user'), cursor='update_time', every='1m')\n@dataset\nclass User:\n    uid: int = field(key=True)\n    email: str\n    ...\n```\n\n<Hint type=\"warning\">If you see a `Cannot create a PoolableConnectionFactory`error, try setting **`jdbc_params` ** to **** `enabledTLSProtocols=TLSv1.2`&#x20;</Hint>\n\nTODO\n\n### Postgres\n\n```python\npostgres = sources.Postgres(\n    name='py_psql_src',\n    host=\"my-favourite-postgres.us-west-2.rds.amazonaws.com\",\n    db_name=\"some_database_name\",\n    username=\"admin\",\n    password=\"password\",\n)\n\n@source(postgres.table('user'), cursor='update_time', every='1m')\n@dataset\nclass User:\n    uid: int\n    ...\n\n```\n\n<Hint type=\"warning\">If you see a `Cannot create a PoolableConnectionFactory`error, try setting **`jdbc_params` ** to **** `enabledTLSProtocols=TLSv1.2`&#x20;</Hint>\n\n\n\nTODO\n\n### S3\n\nThe following fields need to be defined on the source:\n\n1. **`name`** - A name to identify the source. The name should be unique across all sources.\n2. that you don't need to replicate.\n3. **`aws_access_key_id`** - In order to access private Buckets stored on AWS S3, this connector requires credentials with the proper permissions. If accessing publicly available data, this field is not required.\n4. **`aws_secret_access_key` **_**-**_ In order to access private S3 Buckets, this connector requires credentials with the proper permissions. If accessing publicly available data, this field is not required.\n\nAnd the following fields need to be defined on the bucket:\n\n1. **`bucket`** - Name of the S3 bucket where the file(s) exist.\n2. **`path_prefix`** (optional)- By providing a path-like prefix (e.g., `myFolder/thisTable/`) under which all the relevant files sit, we can optimize finding these in S3. This is optional but recommended if your bucket contains many folders/files&#x20;\n3. **`format` ** (optional) **-** The format of the files you'd like to replicate. You can choose between CSV (default), Avro, and Parquet.&#x20;\n4. **`delimiter`** (optional) - the character delimiting individual cells in the CSV data. The default value is `\",\"` and if overridden, this can only be a 1-character string. For example, to use tab-delimited data enter `\"\\t\"`.\n\n```python\ns3 = sources.S3(\n    name='ratings_source',\n    aws_access_key_id=\"<SOME_ACCESS_KEY>\",\n    aws_secret_access_key=\"<SOME_SECRET_ACCESS_KEY>\",\n)\n\n@source(s3.bucket(\"engagement\", prefix=\"notion\"), every=\"30m\")\n@meta(owner='abc@email.com')\n@dataset\nclass User:\n    uid: int = field(key=True)\n    email: str\n    ...\n```\n\nFennel uses  `file_last_modified` property exported by S3 to track what data has been seen so far and hence a cursor field doesn't need to be specified.\n\n\n\n### BigQuery\n\n**Obtaining the credentials**\n\nInterfacing with BigQuery requires credentials for a [Service Account](https://cloud.google.com/iam/docs/service-accounts) with the \"BigQuery User\" and \"BigQuery Data Editor\" roles, which grants permissions to run BigQuery jobs, write to BigQuery Datasets, and read table metadata. It is highly recommended that this Service Account is exclusive to Fennel for ease of permissions and auditing. However, you can also use a pre-existing Service Account if you already have one with the correct permissions.\n\nThe easiest way to create a Service Account is to follow GCP's guide for [Creating a Service Account](https://cloud.google.com/iam/docs/creating-managing-service-accounts). Once you've created the Service Account, make sure to keep its ID handy, as you will need to reference it when granting roles. Service Account IDs typically take the form `<account-name>@<project-name>.iam.gserviceaccount.com`\n\nThen, add the service account as a Member of your Google Cloud Project with the \"BigQuery User\" role. To do this, follow the instructions for [Granting Access](https://cloud.google.com/iam/docs/granting-changing-revoking-access#granting-console) in the Google documentation. The email address of the member you are adding is the same as the Service Account ID you just created.\n\nAt this point, you should have a service account with the \"BigQuery User\" project-level permission.\n\nFor Service Account Key JSON, enter the Google Cloud [Service Account Key in JSON format](https://cloud.google.com/iam/docs/creating-managing-service-account-keys).\n\nTODO\n\n### Kafka\n\nTODO\n\n### Redshift\n\nTODO\n\n### Snowflake\n\nThe following fields need to be defined:\n\n1. **`name`** - A name to identify the source. The name should be unique across all sources.\n2. **`host`** - The host domain of the Snowflake instance (must include the account, region and cloud environment, and end with snowflakecomputing.com). Example: `accountname.us-east-2.aws.snowflakecomputing.com`.\n3. **`role`** - The role that Fennel should use to access Snowflake.\n4. **`warehouse`** - The warehouse that Fennel should use to access Snowflake\n5. **`db_name` **_**-**_ The database where the required data resides.\n6. **`schema`** - The default schema used as the target schema for all statements issued from the connection that do not explicitly specify a schema name.\n7. **`username`**  - The username that should be used to access Snowflake. Please note that the username should have the required permissions to assume the role provided.\n8. **`password` ** **-** The password associated with the username.\n\n```python\nfrom fennel import sources\n\nsf_src = sources.Snowflake(\n    name = \"snowflake_src\",\n    host=\"nhb38793.us-west-2.snowflakecomputing.com\", \n    warehouse=\"TEST\",\n    schema=\"PUBLIC\",\n    role=\"ACCOUNTADMIN\",\n    username=\"<username>\",\n    password=\"<password>\",\n)\n```\n\n<Hint type=\"info\">Currently, Fennel only supports OAuth 1 (username and password) authentication. We are happy to prioritize support for OAuth 2.0 if needed - if so, please talk to us!</Hint>\n\nTODO\n\n"},"misc/python-environment":{"title":"Python Environment","slug":"python-environment","path":"/Users/lukesmetham/Code/fennel/turbo/apps/docs/.content/md/misc/python-environment.md","content":"# Python Environment\n\nBy default, Fennel runs on Python 3.9 and comes pre-installed with the following pip packages:\n\n```\ncharset-normalizer==3.0.1\ncloudpickle==2.2.0\nfrozenlist==1.3.3 \nidna==3.4 \njmespath==1.0.1 \nmpmath==1.2.1 \nmultidict==6.0.4 \nnltk==3.8.1 \nnumpy==1.24.1 \npandas==1.5.2 \npyarrow==10.0.1 \npytz==2022.7.1 \npyparsing==3.0.9 \npython-dateutil==2.8.2 \npytz==2022.7.1 \nregex==2022.10.31 \nscikit-learn==1.2.0 \nscipy==1.10.0 \nsix==1.16.0 \nstatsmodels==0.13.5 \nsympy==1.11.1 \ntyping-extensions==4.4.0 \nyarl==1.8.2\n```\n\nIf you want to run on some other Python version and/or install any specific pip packages with specific versions, please talk to the Fennel team.&#x20;\n\nIn the near future, Fennel will also allow you to configure your environment on your own using Fennel console without dependency on Fennel team.\n"},"misc/troubleshooting-guide":{"title":"Troubleshooting Guide","slug":"troubleshooting-guide","path":"/Users/lukesmetham/Code/fennel/turbo/apps/docs/.content/md/misc/troubleshooting-guide.md","content":"# Troubleshooting Guide\n\n<details>\n\n<summary>I am having issues in connecting my MySQL DB to Fennel</summary>\n\nSome users have reported that they could not connect to Amazon RDS MySQL or MariaDB. This can be diagnosed with the error message: `Cannot create a PoolableConnectionFactory`. To solve this issue please set **`jdbc_params` ** to **** `enabledTLSProtocols=TLSv1.2`&#x20;\n\n</details>\n\n"}}
